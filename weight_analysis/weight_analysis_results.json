{
  "analysis_summary": {
    "cpt": {
      "total_layers": 435,
      "mean_change": 2.2857959163676607,
      "median_change": 0.8909960389137268,
      "max_change": 28.570758819580078,
      "std_change": 3.357706050559547,
      "total_params_affected": 3397103616
    },
    "sft": {
      "total_layers": 435,
      "mean_change": 2.32983608055631,
      "median_change": 0.9074947834014893,
      "max_change": 30.050071716308594,
      "std_change": 3.4431763734654734,
      "total_params_affected": 3397103616
    },
    "cpt_to_sft": {
      "total_layers": 435,
      "mean_change": 0.358298820791928,
      "median_change": 0.15240661799907684,
      "max_change": 4.074172019958496,
      "std_change": 0.4852461677448074,
      "total_params_affected": 3397103616
    }
  },
  "layer_details": {
    "cpt": {
      "lm_head.weight": {
        "magnitude": 28.570758819580078,
        "relative": 0.07029214166724292,
        "shape": [
          151936,
          2048
        ],
        "param_count": 311164928,
        "layer_type": "output"
      },
      "model.embed_tokens.weight": {
        "magnitude": 28.570758819580078,
        "relative": 0.07029214166724292,
        "shape": [
          151936,
          2048
        ],
        "param_count": 311164928,
        "layer_type": "embedding"
      },
      "model.layers.0.input_layernorm.weight": {
        "magnitude": 0.013354291208088398,
        "relative": 0.0008422813185060433,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "layernorm"
      },
      "model.layers.0.mlp.down_proj.weight": {
        "magnitude": 2.4262900352478027,
        "relative": 0.020219195464039084,
        "shape": [
          2048,
          11008
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.0.mlp.gate_proj.weight": {
        "magnitude": 2.2190258502960205,
        "relative": 0.0174479172120691,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.0.mlp.up_proj.weight": {
        "magnitude": 2.148005247116089,
        "relative": 0.01920432684462268,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.0.post_attention_layernorm.weight": {
        "magnitude": 0.0415964312851429,
        "relative": 0.002229921741262914,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.0.self_attn.k_proj.bias": {
        "magnitude": 0.0476350374519825,
        "relative": 0.0001940738841346831,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.0.self_attn.k_proj.weight": {
        "magnitude": 0.4198608100414276,
        "relative": 0.011359329024842464,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.0.self_attn.o_proj.weight": {
        "magnitude": 0.8195010423660278,
        "relative": 0.01855349652303119,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.0.self_attn.q_proj.bias": {
        "magnitude": 0.03289058059453964,
        "relative": 0.0005533717676945747,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.0.self_attn.q_proj.weight": {
        "magnitude": 1.2589175701141357,
        "relative": 0.01680118026832372,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.0.self_attn.v_proj.bias": {
        "magnitude": 0.003878035582602024,
        "relative": 0.0014721395962966027,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.0.self_attn.v_proj.weight": {
        "magnitude": 0.22702161967754364,
        "relative": 0.018985979665445372,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.1.input_layernorm.weight": {
        "magnitude": 0.030779924243688583,
        "relative": 0.002461198940115933,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "layernorm"
      },
      "model.layers.1.mlp.down_proj.weight": {
        "magnitude": 1.0869611501693726,
        "relative": 0.02479640661262799,
        "shape": [
          2048,
          11008
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.1.mlp.gate_proj.weight": {
        "magnitude": 1.299285650253296,
        "relative": 0.022826778526401893,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.1.mlp.up_proj.weight": {
        "magnitude": 0.8447265625,
        "relative": 0.018788550824476374,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.1.post_attention_layernorm.weight": {
        "magnitude": 0.00949068646878004,
        "relative": 0.00016131497590215034,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.1.self_attn.k_proj.bias": {
        "magnitude": 0.01942792534828186,
        "relative": 0.0002689816447057932,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.1.self_attn.k_proj.weight": {
        "magnitude": 0.5956076383590698,
        "relative": 0.019422018803056367,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.1.self_attn.o_proj.weight": {
        "magnitude": 1.2911570072174072,
        "relative": 0.02828107950064859,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.1.self_attn.q_proj.bias": {
        "magnitude": 0.04176584258675575,
        "relative": 0.0012312008174635229,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.1.self_attn.q_proj.weight": {
        "magnitude": 2.043487310409546,
        "relative": 0.03455933484391014,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.1.self_attn.v_proj.bias": {
        "magnitude": 0.004375949967652559,
        "relative": 0.0018085480523116302,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.1.self_attn.v_proj.weight": {
        "magnitude": 0.4191141724586487,
        "relative": 0.0313762892497884,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.10.input_layernorm.weight": {
        "magnitude": 0.0445580929517746,
        "relative": 0.0015191449910085502,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "layernorm"
      },
      "model.layers.10.mlp.down_proj.weight": {
        "magnitude": 4.590882301330566,
        "relative": 0.039794367655915895,
        "shape": [
          2048,
          11008
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.10.mlp.gate_proj.weight": {
        "magnitude": 6.106027126312256,
        "relative": 0.0420418286727367,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.10.mlp.up_proj.weight": {
        "magnitude": 4.337299823760986,
        "relative": 0.036659874144557195,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.10.post_attention_layernorm.weight": {
        "magnitude": 0.039451394230127335,
        "relative": 0.0009168466876377929,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.10.self_attn.k_proj.bias": {
        "magnitude": 0.016357285901904106,
        "relative": 0.0016362538371213326,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.10.self_attn.k_proj.weight": {
        "magnitude": 0.8835800290107727,
        "relative": 0.04437176871963677,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.10.self_attn.o_proj.weight": {
        "magnitude": 2.2605676651000977,
        "relative": 0.04408274200324455,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.10.self_attn.q_proj.bias": {
        "magnitude": 0.04019666463136673,
        "relative": 0.0006305079965362384,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.10.self_attn.q_proj.weight": {
        "magnitude": 2.7272584438323975,
        "relative": 0.050345499496606036,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.10.self_attn.v_proj.bias": {
        "magnitude": 0.004914998542517424,
        "relative": 0.004968156520563425,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.10.self_attn.v_proj.weight": {
        "magnitude": 0.8087829351425171,
        "relative": 0.041385651056268395,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.11.input_layernorm.weight": {
        "magnitude": 0.06534767150878906,
        "relative": 0.002234184767314194,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "layernorm"
      },
      "model.layers.11.mlp.down_proj.weight": {
        "magnitude": 5.180627346038818,
        "relative": 0.043628827448500246,
        "shape": [
          2048,
          11008
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.11.mlp.gate_proj.weight": {
        "magnitude": 6.30275297164917,
        "relative": 0.04475389296489008,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.11.mlp.up_proj.weight": {
        "magnitude": 4.912435054779053,
        "relative": 0.04049308734082028,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.11.post_attention_layernorm.weight": {
        "magnitude": 0.051015712320804596,
        "relative": 0.0014886249731847134,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.11.self_attn.k_proj.bias": {
        "magnitude": 0.01838498003780842,
        "relative": 0.0016481589501432582,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.11.self_attn.k_proj.weight": {
        "magnitude": 1.0568174123764038,
        "relative": 0.051958307375210074,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.11.self_attn.o_proj.weight": {
        "magnitude": 2.6758906841278076,
        "relative": 0.05042578351024205,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.11.self_attn.q_proj.bias": {
        "magnitude": 0.04650937765836716,
        "relative": 0.0006966360933293602,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.11.self_attn.q_proj.weight": {
        "magnitude": 3.233635187149048,
        "relative": 0.05845031714077507,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.11.self_attn.v_proj.bias": {
        "magnitude": 0.007125831209123135,
        "relative": 0.00646657366159082,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.11.self_attn.v_proj.weight": {
        "magnitude": 0.8909960389137268,
        "relative": 0.04527010348784153,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.12.input_layernorm.weight": {
        "magnitude": 0.06564615666866302,
        "relative": 0.0020794369050091957,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "layernorm"
      },
      "model.layers.12.mlp.down_proj.weight": {
        "magnitude": 5.4524126052856445,
        "relative": 0.046289864973016764,
        "shape": [
          2048,
          11008
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.12.mlp.gate_proj.weight": {
        "magnitude": 7.143093585968018,
        "relative": 0.049835612685921765,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.12.mlp.up_proj.weight": {
        "magnitude": 5.180022716522217,
        "relative": 0.0429174982040957,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.12.post_attention_layernorm.weight": {
        "magnitude": 0.05383508652448654,
        "relative": 0.0014156483812621577,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.12.self_attn.k_proj.bias": {
        "magnitude": 0.01535266637802124,
        "relative": 0.0013315862895863654,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.12.self_attn.k_proj.weight": {
        "magnitude": 1.0566301345825195,
        "relative": 0.04949547498563335,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.12.self_attn.o_proj.weight": {
        "magnitude": 2.6246438026428223,
        "relative": 0.0496002353636189,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.12.self_attn.q_proj.bias": {
        "magnitude": 0.05070393905043602,
        "relative": 0.0008563856191461394,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.12.self_attn.q_proj.weight": {
        "magnitude": 3.325493097305298,
        "relative": 0.06013827517932672,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.12.self_attn.v_proj.bias": {
        "magnitude": 0.006676968187093735,
        "relative": 0.0039174668757115914,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.12.self_attn.v_proj.weight": {
        "magnitude": 0.9866857528686523,
        "relative": 0.051939661679977864,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.13.input_layernorm.weight": {
        "magnitude": 0.05354703590273857,
        "relative": 0.0019443007968248888,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "layernorm"
      },
      "model.layers.13.mlp.down_proj.weight": {
        "magnitude": 6.22584342956543,
        "relative": 0.04956707742276141,
        "shape": [
          2048,
          11008
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.13.mlp.gate_proj.weight": {
        "magnitude": 7.089761734008789,
        "relative": 0.05439838013138343,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.13.mlp.up_proj.weight": {
        "magnitude": 5.828617095947266,
        "relative": 0.0455563630247815,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.13.post_attention_layernorm.weight": {
        "magnitude": 0.06085352599620819,
        "relative": 0.0017923683625497816,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.13.self_attn.k_proj.bias": {
        "magnitude": 0.014876593835651875,
        "relative": 0.0014354101334923956,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.13.self_attn.k_proj.weight": {
        "magnitude": 1.0132560729980469,
        "relative": 0.04048460691227838,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.13.self_attn.o_proj.weight": {
        "magnitude": 2.354659080505371,
        "relative": 0.04947220065866609,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.13.self_attn.q_proj.bias": {
        "magnitude": 0.03631982207298279,
        "relative": 0.0005120086152407398,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.13.self_attn.q_proj.weight": {
        "magnitude": 3.199345827102661,
        "relative": 0.05493993851364828,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.13.self_attn.v_proj.bias": {
        "magnitude": 0.006840932182967663,
        "relative": 0.005213210760692996,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.13.self_attn.v_proj.weight": {
        "magnitude": 0.8691534399986267,
        "relative": 0.05487172904476627,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.14.input_layernorm.weight": {
        "magnitude": 0.07328441739082336,
        "relative": 0.0024287796638815767,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "layernorm"
      },
      "model.layers.14.mlp.down_proj.weight": {
        "magnitude": 6.790241718292236,
        "relative": 0.05449773417205997,
        "shape": [
          2048,
          11008
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.14.mlp.gate_proj.weight": {
        "magnitude": 8.131426811218262,
        "relative": 0.06245668680720211,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.14.mlp.up_proj.weight": {
        "magnitude": 6.424949645996094,
        "relative": 0.05032708712773821,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.14.post_attention_layernorm.weight": {
        "magnitude": 0.06494908779859543,
        "relative": 0.001858438496716268,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.14.self_attn.k_proj.bias": {
        "magnitude": 0.025873739272356033,
        "relative": 0.0022737769917149063,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.14.self_attn.k_proj.weight": {
        "magnitude": 1.2081712484359741,
        "relative": 0.05422940765715896,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.14.self_attn.o_proj.weight": {
        "magnitude": 2.8675808906555176,
        "relative": 0.056272336270530736,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.14.self_attn.q_proj.bias": {
        "magnitude": 0.04737840220332146,
        "relative": 0.0007174302949921834,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.14.self_attn.q_proj.weight": {
        "magnitude": 3.8241403102874756,
        "relative": 0.0660795998429064,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.14.self_attn.v_proj.bias": {
        "magnitude": 0.008372348733246326,
        "relative": 0.008445700503577222,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.14.self_attn.v_proj.weight": {
        "magnitude": 0.9296325445175171,
        "relative": 0.050231592937108764,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.15.input_layernorm.weight": {
        "magnitude": 0.06465902179479599,
        "relative": 0.0019670798086926047,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "layernorm"
      },
      "model.layers.15.mlp.down_proj.weight": {
        "magnitude": 7.2249250411987305,
        "relative": 0.05730804955878688,
        "shape": [
          2048,
          11008
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.15.mlp.gate_proj.weight": {
        "magnitude": 8.641501426696777,
        "relative": 0.06857193057789653,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.15.mlp.up_proj.weight": {
        "magnitude": 6.9171528816223145,
        "relative": 0.05343698129362221,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.15.post_attention_layernorm.weight": {
        "magnitude": 0.07298953086137772,
        "relative": 0.002065939800532921,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.15.self_attn.k_proj.bias": {
        "magnitude": 0.02324756793677807,
        "relative": 0.002058178496854913,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.15.self_attn.k_proj.weight": {
        "magnitude": 1.1555380821228027,
        "relative": 0.05253820506246974,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.15.self_attn.o_proj.weight": {
        "magnitude": 2.733412027359009,
        "relative": 0.05497186243937152,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.15.self_attn.q_proj.bias": {
        "magnitude": 0.03798256069421768,
        "relative": 0.0005629334064742854,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.15.self_attn.q_proj.weight": {
        "magnitude": 3.608802080154419,
        "relative": 0.06301251219400826,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.15.self_attn.v_proj.bias": {
        "magnitude": 0.008395230397582054,
        "relative": 0.006879679796710209,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.15.self_attn.v_proj.weight": {
        "magnitude": 0.9749616980552673,
        "relative": 0.05511226196088939,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.16.input_layernorm.weight": {
        "magnitude": 0.08176521956920624,
        "relative": 0.0021614821121320755,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "layernorm"
      },
      "model.layers.16.mlp.down_proj.weight": {
        "magnitude": 7.914787292480469,
        "relative": 0.06488059242917014,
        "shape": [
          2048,
          11008
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.16.mlp.gate_proj.weight": {
        "magnitude": 9.52012825012207,
        "relative": 0.07426062657894243,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.16.mlp.up_proj.weight": {
        "magnitude": 7.596617221832275,
        "relative": 0.0603929311743297,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.16.post_attention_layernorm.weight": {
        "magnitude": 0.08425646275281906,
        "relative": 0.0023412377781113014,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.16.self_attn.k_proj.bias": {
        "magnitude": 0.033813510090112686,
        "relative": 0.0020229435862572052,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.16.self_attn.k_proj.weight": {
        "magnitude": 1.459109902381897,
        "relative": 0.06514256904260983,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.16.self_attn.o_proj.weight": {
        "magnitude": 3.3612513542175293,
        "relative": 0.06578498181385044,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.16.self_attn.q_proj.bias": {
        "magnitude": 0.04986610263586044,
        "relative": 0.0008643967788444624,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.16.self_attn.q_proj.weight": {
        "magnitude": 4.365472793579102,
        "relative": 0.0752696832049409,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.16.self_attn.v_proj.bias": {
        "magnitude": 0.006714198272675276,
        "relative": 0.009916960381716376,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.16.self_attn.v_proj.weight": {
        "magnitude": 1.0706127882003784,
        "relative": 0.05860142142261561,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.17.input_layernorm.weight": {
        "magnitude": 0.06735454499721527,
        "relative": 0.0016271835486220904,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "layernorm"
      },
      "model.layers.17.mlp.down_proj.weight": {
        "magnitude": 8.013816833496094,
        "relative": 0.06582481633268528,
        "shape": [
          2048,
          11008
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.17.mlp.gate_proj.weight": {
        "magnitude": 9.457344055175781,
        "relative": 0.07598134853927606,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.17.mlp.up_proj.weight": {
        "magnitude": 7.887706279754639,
        "relative": 0.06271276525230089,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.17.post_attention_layernorm.weight": {
        "magnitude": 0.08908171206712723,
        "relative": 0.0024436659517459423,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.17.self_attn.k_proj.bias": {
        "magnitude": 0.024834193289279938,
        "relative": 0.0029813315756249898,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.17.self_attn.k_proj.weight": {
        "magnitude": 1.2129427194595337,
        "relative": 0.059813200147405286,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.17.self_attn.o_proj.weight": {
        "magnitude": 2.5007128715515137,
        "relative": 0.05156611938892363,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.17.self_attn.q_proj.bias": {
        "magnitude": 0.041636619716882706,
        "relative": 0.0007037366483127047,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.17.self_attn.q_proj.weight": {
        "magnitude": 3.623445749282837,
        "relative": 0.06661730408356142,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.17.self_attn.v_proj.bias": {
        "magnitude": 0.007006913423538208,
        "relative": 0.005136118136782376,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.17.self_attn.v_proj.weight": {
        "magnitude": 0.9232218265533447,
        "relative": 0.05449438625963421,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.18.input_layernorm.weight": {
        "magnitude": 0.06776213645935059,
        "relative": 0.0015744439810153738,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "layernorm"
      },
      "model.layers.18.mlp.down_proj.weight": {
        "magnitude": 8.15124225616455,
        "relative": 0.06789154207399413,
        "shape": [
          2048,
          11008
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.18.mlp.gate_proj.weight": {
        "magnitude": 9.979236602783203,
        "relative": 0.07950260124517715,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.18.mlp.up_proj.weight": {
        "magnitude": 8.141852378845215,
        "relative": 0.06563495707796181,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.18.post_attention_layernorm.weight": {
        "magnitude": 0.08484159409999847,
        "relative": 0.0022800475634797523,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.18.self_attn.k_proj.bias": {
        "magnitude": 0.03335811197757721,
        "relative": 0.0008844290794752187,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.18.self_attn.k_proj.weight": {
        "magnitude": 1.3592920303344727,
        "relative": 0.06557757562968218,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.18.self_attn.o_proj.weight": {
        "magnitude": 2.7552058696746826,
        "relative": 0.05613339813179388,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.18.self_attn.q_proj.bias": {
        "magnitude": 0.04686356335878372,
        "relative": 0.0007354775359511054,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.18.self_attn.q_proj.weight": {
        "magnitude": 4.133866786956787,
        "relative": 0.0746979940142106,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.18.self_attn.v_proj.bias": {
        "magnitude": 0.005838300567120314,
        "relative": 0.008197688748313899,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.18.self_attn.v_proj.weight": {
        "magnitude": 0.975118100643158,
        "relative": 0.053768793154275005,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.19.input_layernorm.weight": {
        "magnitude": 0.09382564574480057,
        "relative": 0.0019850484428223097,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "layernorm"
      },
      "model.layers.19.mlp.down_proj.weight": {
        "magnitude": 8.43818473815918,
        "relative": 0.069537374934981,
        "shape": [
          2048,
          11008
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.19.mlp.gate_proj.weight": {
        "magnitude": 9.942479133605957,
        "relative": 0.08281180932840461,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.19.mlp.up_proj.weight": {
        "magnitude": 8.407661437988281,
        "relative": 0.06856803912441237,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.19.post_attention_layernorm.weight": {
        "magnitude": 0.07345671951770782,
        "relative": 0.0018965293603634367,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.19.self_attn.k_proj.bias": {
        "magnitude": 0.024865612387657166,
        "relative": 0.0009419289487116213,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.19.self_attn.k_proj.weight": {
        "magnitude": 1.236989140510559,
        "relative": 0.05521738923445849,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.19.self_attn.o_proj.weight": {
        "magnitude": 2.569657325744629,
        "relative": 0.05434821895586699,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.19.self_attn.q_proj.bias": {
        "magnitude": 0.035153500735759735,
        "relative": 0.0005595962439877443,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.19.self_attn.q_proj.weight": {
        "magnitude": 3.910524368286133,
        "relative": 0.07057509669476063,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.19.self_attn.v_proj.bias": {
        "magnitude": 0.008357975631952286,
        "relative": 0.010014564444987812,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.19.self_attn.v_proj.weight": {
        "magnitude": 0.969286322593689,
        "relative": 0.06024755652808526,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.2.input_layernorm.weight": {
        "magnitude": 0.024915043264627457,
        "relative": 0.0014008409336131755,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "layernorm"
      },
      "model.layers.2.mlp.down_proj.weight": {
        "magnitude": 1.359025001525879,
        "relative": 0.023177521758219407,
        "shape": [
          2048,
          11008
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.2.mlp.gate_proj.weight": {
        "magnitude": 1.5216381549835205,
        "relative": 0.02679966735120668,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.2.mlp.up_proj.weight": {
        "magnitude": 1.2889119386672974,
        "relative": 0.01942170069860573,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.2.post_attention_layernorm.weight": {
        "magnitude": 0.0025318043772131205,
        "relative": 4.19629115505357e-05,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.2.self_attn.k_proj.bias": {
        "magnitude": 0.026607312262058258,
        "relative": 0.0005667818520311311,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.2.self_attn.k_proj.weight": {
        "magnitude": 0.6061174273490906,
        "relative": 0.02238389887089076,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.2.self_attn.o_proj.weight": {
        "magnitude": 1.2975428104400635,
        "relative": 0.028393948457572947,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.2.self_attn.q_proj.bias": {
        "magnitude": 0.04837857186794281,
        "relative": 0.0014386757195565091,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.2.self_attn.q_proj.weight": {
        "magnitude": 2.2102773189544678,
        "relative": 0.03826386108616289,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.2.self_attn.v_proj.bias": {
        "magnitude": 0.0037615669425576925,
        "relative": 0.01022815796468167,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.2.self_attn.v_proj.weight": {
        "magnitude": 0.41804468631744385,
        "relative": 0.030546928031515604,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.20.input_layernorm.weight": {
        "magnitude": 0.0612793006002903,
        "relative": 0.0010632194575618888,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "layernorm"
      },
      "model.layers.20.mlp.down_proj.weight": {
        "magnitude": 8.751235008239746,
        "relative": 0.07109052635294187,
        "shape": [
          2048,
          11008
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.20.mlp.gate_proj.weight": {
        "magnitude": 10.41576862335205,
        "relative": 0.08369005565400442,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.20.mlp.up_proj.weight": {
        "magnitude": 8.772167205810547,
        "relative": 0.06895257923617369,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.20.post_attention_layernorm.weight": {
        "magnitude": 0.09161488711833954,
        "relative": 0.002410532598395266,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.20.self_attn.k_proj.bias": {
        "magnitude": 0.030649054795503616,
        "relative": 0.001004374390372915,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.20.self_attn.k_proj.weight": {
        "magnitude": 1.3174175024032593,
        "relative": 0.07181336501849082,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.20.self_attn.o_proj.weight": {
        "magnitude": 2.835479497909546,
        "relative": 0.054396203318612134,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.20.self_attn.q_proj.bias": {
        "magnitude": 0.04646047577261925,
        "relative": 0.0007285120405167974,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.20.self_attn.q_proj.weight": {
        "magnitude": 4.109536170959473,
        "relative": 0.07735837464205071,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.20.self_attn.v_proj.bias": {
        "magnitude": 0.004314119461923838,
        "relative": 0.0033268704069535932,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.20.self_attn.v_proj.weight": {
        "magnitude": 0.9293233156204224,
        "relative": 0.05250080757882044,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.21.input_layernorm.weight": {
        "magnitude": 0.09060348570346832,
        "relative": 0.001580543360412211,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "layernorm"
      },
      "model.layers.21.mlp.down_proj.weight": {
        "magnitude": 8.90335464477539,
        "relative": 0.0736280814941775,
        "shape": [
          2048,
          11008
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.21.mlp.gate_proj.weight": {
        "magnitude": 10.824860572814941,
        "relative": 0.08492697799991672,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.21.mlp.up_proj.weight": {
        "magnitude": 8.928425788879395,
        "relative": 0.07131991712603311,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.21.post_attention_layernorm.weight": {
        "magnitude": 0.10294749587774277,
        "relative": 0.002642664897822577,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.21.self_attn.k_proj.bias": {
        "magnitude": 0.03378913179039955,
        "relative": 0.0015297913561770913,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.21.self_attn.k_proj.weight": {
        "magnitude": 1.4831633567810059,
        "relative": 0.073839487420051,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.21.self_attn.o_proj.weight": {
        "magnitude": 2.9827206134796143,
        "relative": 0.058938034016129896,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.21.self_attn.q_proj.bias": {
        "magnitude": 0.047962039709091187,
        "relative": 0.000989326285426402,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.21.self_attn.q_proj.weight": {
        "magnitude": 4.299247741699219,
        "relative": 0.07963113485444333,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.21.self_attn.v_proj.bias": {
        "magnitude": 0.005976343061774969,
        "relative": 0.008200497221503076,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.21.self_attn.v_proj.weight": {
        "magnitude": 1.1106834411621094,
        "relative": 0.0612978252396097,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.22.input_layernorm.weight": {
        "magnitude": 0.11219530552625656,
        "relative": 0.0021933250282170236,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "layernorm"
      },
      "model.layers.22.mlp.down_proj.weight": {
        "magnitude": 9.080761909484863,
        "relative": 0.0749044007768026,
        "shape": [
          2048,
          11008
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.22.mlp.gate_proj.weight": {
        "magnitude": 10.912964820861816,
        "relative": 0.08657360777462664,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.22.mlp.up_proj.weight": {
        "magnitude": 8.986676216125488,
        "relative": 0.07157773635318054,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.22.post_attention_layernorm.weight": {
        "magnitude": 0.09948485344648361,
        "relative": 0.0025132907414454515,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.22.self_attn.k_proj.bias": {
        "magnitude": 0.026212645694613457,
        "relative": 0.00193076293662143,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.22.self_attn.k_proj.weight": {
        "magnitude": 1.422870397567749,
        "relative": 0.07531906869378711,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.22.self_attn.o_proj.weight": {
        "magnitude": 2.9580461978912354,
        "relative": 0.05733275985961156,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.22.self_attn.q_proj.bias": {
        "magnitude": 0.04238627851009369,
        "relative": 0.0007739447862548793,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.22.self_attn.q_proj.weight": {
        "magnitude": 4.2542500495910645,
        "relative": 0.07831778880989838,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.22.self_attn.v_proj.bias": {
        "magnitude": 0.008172617293894291,
        "relative": 0.008953153856153521,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.22.self_attn.v_proj.weight": {
        "magnitude": 1.07241690158844,
        "relative": 0.05640902979434643,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.23.input_layernorm.weight": {
        "magnitude": 0.09628771245479584,
        "relative": 0.0020251549771576507,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "layernorm"
      },
      "model.layers.23.mlp.down_proj.weight": {
        "magnitude": 8.775431632995605,
        "relative": 0.07218189065815726,
        "shape": [
          2048,
          11008
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.23.mlp.gate_proj.weight": {
        "magnitude": 10.514274597167969,
        "relative": 0.0862187736707996,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.23.mlp.up_proj.weight": {
        "magnitude": 8.98634147644043,
        "relative": 0.0709768143474222,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.23.post_attention_layernorm.weight": {
        "magnitude": 0.09187489748001099,
        "relative": 0.0023108005845534634,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.23.self_attn.k_proj.bias": {
        "magnitude": 0.03790057450532913,
        "relative": 0.001943062339529178,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.23.self_attn.k_proj.weight": {
        "magnitude": 1.41390061378479,
        "relative": 0.06486957713069286,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.23.self_attn.o_proj.weight": {
        "magnitude": 2.740138053894043,
        "relative": 0.057967601401276304,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.23.self_attn.q_proj.bias": {
        "magnitude": 0.03130778670310974,
        "relative": 0.00048091546296838075,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.23.self_attn.q_proj.weight": {
        "magnitude": 4.079936981201172,
        "relative": 0.07406833631312112,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.23.self_attn.v_proj.bias": {
        "magnitude": 0.008043571375310421,
        "relative": 0.007954708432318937,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.23.self_attn.v_proj.weight": {
        "magnitude": 1.0973058938980103,
        "relative": 0.06466761479128866,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.24.input_layernorm.weight": {
        "magnitude": 0.09076286852359772,
        "relative": 0.001956388166286862,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "layernorm"
      },
      "model.layers.24.mlp.down_proj.weight": {
        "magnitude": 8.699715614318848,
        "relative": 0.07207833439783268,
        "shape": [
          2048,
          11008
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.24.mlp.gate_proj.weight": {
        "magnitude": 10.470571517944336,
        "relative": 0.08606998591779771,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.24.mlp.up_proj.weight": {
        "magnitude": 8.79556655883789,
        "relative": 0.07027541428377165,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.24.post_attention_layernorm.weight": {
        "magnitude": 0.09646737575531006,
        "relative": 0.0024711580399318003,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.24.self_attn.k_proj.bias": {
        "magnitude": 0.0387408472597599,
        "relative": 0.0028161635446842157,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.24.self_attn.k_proj.weight": {
        "magnitude": 1.5289074182510376,
        "relative": 0.0774003488760025,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.24.self_attn.o_proj.weight": {
        "magnitude": 3.2762327194213867,
        "relative": 0.06512315960297291,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.24.self_attn.q_proj.bias": {
        "magnitude": 0.04248041659593582,
        "relative": 0.0005778348748998539,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.24.self_attn.q_proj.weight": {
        "magnitude": 4.599453449249268,
        "relative": 0.0840717736324511,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.24.self_attn.v_proj.bias": {
        "magnitude": 0.006218604743480682,
        "relative": 0.008771523895823996,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.24.self_attn.v_proj.weight": {
        "magnitude": 1.0657545328140259,
        "relative": 0.056968476424932045,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.25.input_layernorm.weight": {
        "magnitude": 0.049316029995679855,
        "relative": 0.0009039896866085328,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "layernorm"
      },
      "model.layers.25.mlp.down_proj.weight": {
        "magnitude": 8.311633110046387,
        "relative": 0.06780658571233199,
        "shape": [
          2048,
          11008
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.25.mlp.gate_proj.weight": {
        "magnitude": 10.323164939880371,
        "relative": 0.08398487433748837,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.25.mlp.up_proj.weight": {
        "magnitude": 8.468908309936523,
        "relative": 0.06660960532191287,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.25.post_attention_layernorm.weight": {
        "magnitude": 0.08504508435726166,
        "relative": 0.0021940368871603776,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.25.self_attn.k_proj.bias": {
        "magnitude": 0.029455743730068207,
        "relative": 0.0019939324504828155,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.25.self_attn.k_proj.weight": {
        "magnitude": 1.4361575841903687,
        "relative": 0.09280621795262085,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.25.self_attn.o_proj.weight": {
        "magnitude": 2.7037315368652344,
        "relative": 0.05196033293870984,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.25.self_attn.q_proj.bias": {
        "magnitude": 0.04418216273188591,
        "relative": 0.0007451487288102215,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.25.self_attn.q_proj.weight": {
        "magnitude": 4.324927806854248,
        "relative": 0.0827236761964608,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.25.self_attn.v_proj.bias": {
        "magnitude": 0.004182417877018452,
        "relative": 0.004638917429236249,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.25.self_attn.v_proj.weight": {
        "magnitude": 0.915370762348175,
        "relative": 0.04872251628189637,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.26.input_layernorm.weight": {
        "magnitude": 0.07598894089460373,
        "relative": 0.0015925022188814496,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "layernorm"
      },
      "model.layers.26.mlp.down_proj.weight": {
        "magnitude": 7.820680141448975,
        "relative": 0.06265552373871248,
        "shape": [
          2048,
          11008
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.26.mlp.gate_proj.weight": {
        "magnitude": 10.344715118408203,
        "relative": 0.08468218351655221,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.26.mlp.up_proj.weight": {
        "magnitude": 8.063153266906738,
        "relative": 0.0620330005461475,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.26.post_attention_layernorm.weight": {
        "magnitude": 0.084166020154953,
        "relative": 0.0021573060878607823,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.26.self_attn.k_proj.bias": {
        "magnitude": 0.021750882267951965,
        "relative": 0.0016280197648343706,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.26.self_attn.k_proj.weight": {
        "magnitude": 1.4725443124771118,
        "relative": 0.07797388708928883,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.26.self_attn.o_proj.weight": {
        "magnitude": 3.1645586490631104,
        "relative": 0.05989427314297521,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.26.self_attn.q_proj.bias": {
        "magnitude": 0.0354359969496727,
        "relative": 0.0005136116277529732,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.26.self_attn.q_proj.weight": {
        "magnitude": 4.275574684143066,
        "relative": 0.07954437304403764,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.26.self_attn.v_proj.bias": {
        "magnitude": 0.004739029798656702,
        "relative": 0.0061758288439705995,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.26.self_attn.v_proj.weight": {
        "magnitude": 1.0172110795974731,
        "relative": 0.04859976055547395,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.27.input_layernorm.weight": {
        "magnitude": 0.0402638204395771,
        "relative": 0.000790285333999071,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "layernorm"
      },
      "model.layers.27.mlp.down_proj.weight": {
        "magnitude": 7.345603942871094,
        "relative": 0.05880475732686936,
        "shape": [
          2048,
          11008
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.27.mlp.gate_proj.weight": {
        "magnitude": 9.743552207946777,
        "relative": 0.07816072630989067,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.27.mlp.up_proj.weight": {
        "magnitude": 7.562382698059082,
        "relative": 0.05823059191553439,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.27.post_attention_layernorm.weight": {
        "magnitude": 0.07013599574565887,
        "relative": 0.0017307955425866408,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.27.self_attn.k_proj.bias": {
        "magnitude": 0.018834760412573814,
        "relative": 0.00027386052929941377,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.27.self_attn.k_proj.weight": {
        "magnitude": 1.371202826499939,
        "relative": 0.08544730635544491,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.27.self_attn.o_proj.weight": {
        "magnitude": 2.622532844543457,
        "relative": 0.051551738388140526,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.27.self_attn.q_proj.bias": {
        "magnitude": 0.03360012546181679,
        "relative": 0.0001487365380620373,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.27.self_attn.q_proj.weight": {
        "magnitude": 3.8181493282318115,
        "relative": 0.07476999970372991,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.27.self_attn.v_proj.bias": {
        "magnitude": 0.004070490598678589,
        "relative": 0.0022350397053668564,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.27.self_attn.v_proj.weight": {
        "magnitude": 0.8881178498268127,
        "relative": 0.04448577013940345,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.28.input_layernorm.weight": {
        "magnitude": 0.0655784159898758,
        "relative": 0.0014460883395165688,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "layernorm"
      },
      "model.layers.28.mlp.down_proj.weight": {
        "magnitude": 7.338150978088379,
        "relative": 0.05845347328502599,
        "shape": [
          2048,
          11008
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.28.mlp.gate_proj.weight": {
        "magnitude": 9.388421058654785,
        "relative": 0.07428839602805454,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.28.mlp.up_proj.weight": {
        "magnitude": 7.320296764373779,
        "relative": 0.05624698535928872,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.28.post_attention_layernorm.weight": {
        "magnitude": 0.06847808510065079,
        "relative": 0.001632605615844584,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.28.self_attn.k_proj.bias": {
        "magnitude": 0.026142528280615807,
        "relative": 0.0019725944979555373,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.28.self_attn.k_proj.weight": {
        "magnitude": 1.3176418542861938,
        "relative": 0.07654601578853179,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.28.self_attn.o_proj.weight": {
        "magnitude": 3.724419593811035,
        "relative": 0.07020441328275576,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.28.self_attn.q_proj.bias": {
        "magnitude": 0.03181779384613037,
        "relative": 0.0005093429791342428,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.28.self_attn.q_proj.weight": {
        "magnitude": 4.218994140625,
        "relative": 0.07968875110653327,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.28.self_attn.v_proj.bias": {
        "magnitude": 0.004924132488667965,
        "relative": 0.004165993474460009,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.28.self_attn.v_proj.weight": {
        "magnitude": 0.8647585511207581,
        "relative": 0.039546087886643724,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.29.input_layernorm.weight": {
        "magnitude": 0.047563087195158005,
        "relative": 0.0010003631242624757,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "layernorm"
      },
      "model.layers.29.mlp.down_proj.weight": {
        "magnitude": 6.539109706878662,
        "relative": 0.05180066318305321,
        "shape": [
          2048,
          11008
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.29.mlp.gate_proj.weight": {
        "magnitude": 8.841903686523438,
        "relative": 0.07000029052327315,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.29.mlp.up_proj.weight": {
        "magnitude": 6.553266525268555,
        "relative": 0.05016890504335294,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.29.post_attention_layernorm.weight": {
        "magnitude": 0.06629125773906708,
        "relative": 0.0014949506897735658,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.29.self_attn.k_proj.bias": {
        "magnitude": 0.019381821155548096,
        "relative": 0.0007186869922612297,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.29.self_attn.k_proj.weight": {
        "magnitude": 1.3833420276641846,
        "relative": 0.08130118103780967,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.29.self_attn.o_proj.weight": {
        "magnitude": 3.0466556549072266,
        "relative": 0.05761766219364849,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.29.self_attn.q_proj.bias": {
        "magnitude": 0.029244907200336456,
        "relative": 0.0004938642001961293,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.29.self_attn.q_proj.weight": {
        "magnitude": 3.9734859466552734,
        "relative": 0.07668825432834936,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.29.self_attn.v_proj.bias": {
        "magnitude": 0.004154756665229797,
        "relative": 0.0028135953639298016,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.29.self_attn.v_proj.weight": {
        "magnitude": 0.7716524004936218,
        "relative": 0.03541115218598777,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.3.input_layernorm.weight": {
        "magnitude": 0.012989664450287819,
        "relative": 0.00042055879734001917,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "layernorm"
      },
      "model.layers.3.mlp.down_proj.weight": {
        "magnitude": 1.93007493019104,
        "relative": 0.02369972286038768,
        "shape": [
          2048,
          11008
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.3.mlp.gate_proj.weight": {
        "magnitude": 2.2084197998046875,
        "relative": 0.028082505684489292,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.3.mlp.up_proj.weight": {
        "magnitude": 1.8775365352630615,
        "relative": 0.022037027702413798,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.3.post_attention_layernorm.weight": {
        "magnitude": 0.0019772194791585207,
        "relative": 3.241068163724213e-05,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.3.self_attn.k_proj.bias": {
        "magnitude": 0.013628174550831318,
        "relative": 0.0009244991009450232,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.3.self_attn.k_proj.weight": {
        "magnitude": 0.5677374005317688,
        "relative": 0.02162353029001861,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.3.self_attn.o_proj.weight": {
        "magnitude": 1.54453706741333,
        "relative": 0.03407109873671498,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.3.self_attn.q_proj.bias": {
        "magnitude": 0.028084494173526764,
        "relative": 0.00045514978827784227,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.3.self_attn.q_proj.weight": {
        "magnitude": 1.8584328889846802,
        "relative": 0.031609555773131924,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.3.self_attn.v_proj.bias": {
        "magnitude": 0.004978006239980459,
        "relative": 0.008255426244499951,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.3.self_attn.v_proj.weight": {
        "magnitude": 0.4551439583301544,
        "relative": 0.03820504518328427,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.30.input_layernorm.weight": {
        "magnitude": 0.05065255984663963,
        "relative": 0.0011479098476848157,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "layernorm"
      },
      "model.layers.30.mlp.down_proj.weight": {
        "magnitude": 5.943166732788086,
        "relative": 0.04678963723765595,
        "shape": [
          2048,
          11008
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.30.mlp.gate_proj.weight": {
        "magnitude": 8.254899024963379,
        "relative": 0.0662029742473385,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.30.mlp.up_proj.weight": {
        "magnitude": 6.0039286613464355,
        "relative": 0.04543954478403394,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.30.post_attention_layernorm.weight": {
        "magnitude": 0.0519697330892086,
        "relative": 0.0011251884804467638,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.30.self_attn.k_proj.bias": {
        "magnitude": 0.011540294624865055,
        "relative": 0.0011033787356006036,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.30.self_attn.k_proj.weight": {
        "magnitude": 1.0216065645217896,
        "relative": 0.06687986529397531,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.30.self_attn.o_proj.weight": {
        "magnitude": 2.1867451667785645,
        "relative": 0.04017696504641378,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.30.self_attn.q_proj.bias": {
        "magnitude": 0.02652238868176937,
        "relative": 0.0004130489369321786,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.30.self_attn.q_proj.weight": {
        "magnitude": 3.385014295578003,
        "relative": 0.0639255741888177,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.30.self_attn.v_proj.bias": {
        "magnitude": 0.003040573326870799,
        "relative": 0.001930866992487956,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.30.self_attn.v_proj.weight": {
        "magnitude": 0.6901726722717285,
        "relative": 0.031631061201283686,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.31.input_layernorm.weight": {
        "magnitude": 0.04152418673038483,
        "relative": 0.0009735812872202066,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "layernorm"
      },
      "model.layers.31.mlp.down_proj.weight": {
        "magnitude": 4.95507287979126,
        "relative": 0.03812378953529599,
        "shape": [
          2048,
          11008
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.31.mlp.gate_proj.weight": {
        "magnitude": 7.103753566741943,
        "relative": 0.057614944231046274,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.31.mlp.up_proj.weight": {
        "magnitude": 5.175422668457031,
        "relative": 0.03836995794904857,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.31.post_attention_layernorm.weight": {
        "magnitude": 0.020670173689723015,
        "relative": 0.00041428845773015626,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.31.self_attn.k_proj.bias": {
        "magnitude": 0.02681618742644787,
        "relative": 0.0014745664046742716,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.31.self_attn.k_proj.weight": {
        "magnitude": 0.7969567775726318,
        "relative": 0.050736581304534595,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.31.self_attn.o_proj.weight": {
        "magnitude": 1.620769739151001,
        "relative": 0.029820180036434003,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.31.self_attn.q_proj.bias": {
        "magnitude": 0.022748123854398727,
        "relative": 0.0004202479957131423,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.31.self_attn.q_proj.weight": {
        "magnitude": 2.9308066368103027,
        "relative": 0.054426984034565716,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.31.self_attn.v_proj.bias": {
        "magnitude": 0.003029789077118039,
        "relative": 0.00041094894538974144,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.31.self_attn.v_proj.weight": {
        "magnitude": 0.598604679107666,
        "relative": 0.029170509224036547,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.32.input_layernorm.weight": {
        "magnitude": 0.030325092375278473,
        "relative": 0.0006232835007559885,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "layernorm"
      },
      "model.layers.32.mlp.down_proj.weight": {
        "magnitude": 4.707472324371338,
        "relative": 0.03580794567871309,
        "shape": [
          2048,
          11008
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.32.mlp.gate_proj.weight": {
        "magnitude": 6.315643787384033,
        "relative": 0.05094251575980831,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.32.mlp.up_proj.weight": {
        "magnitude": 4.857978343963623,
        "relative": 0.036058496007158033,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.32.post_attention_layernorm.weight": {
        "magnitude": 0.019629083573818207,
        "relative": 0.00037207034010980084,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.32.self_attn.k_proj.bias": {
        "magnitude": 0.015297618694603443,
        "relative": 0.000874334797241566,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.32.self_attn.k_proj.weight": {
        "magnitude": 0.8636165857315063,
        "relative": 0.06073521373061582,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.32.self_attn.o_proj.weight": {
        "magnitude": 1.471362829208374,
        "relative": 0.024106923097685036,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.32.self_attn.q_proj.bias": {
        "magnitude": 0.02456924505531788,
        "relative": 0.00047976482791808113,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.32.self_attn.q_proj.weight": {
        "magnitude": 2.8852365016937256,
        "relative": 0.05409924592385153,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.32.self_attn.v_proj.bias": {
        "magnitude": 0.0031321109272539616,
        "relative": 0.00041704275805817494,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.32.self_attn.v_proj.weight": {
        "magnitude": 0.39546874165534973,
        "relative": 0.017728617576053585,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.33.input_layernorm.weight": {
        "magnitude": 0.03022618405520916,
        "relative": 0.000549474336063549,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "layernorm"
      },
      "model.layers.33.mlp.down_proj.weight": {
        "magnitude": 4.343369960784912,
        "relative": 0.03272557234736816,
        "shape": [
          2048,
          11008
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.33.mlp.gate_proj.weight": {
        "magnitude": 5.704530239105225,
        "relative": 0.04654644015970941,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.33.mlp.up_proj.weight": {
        "magnitude": 4.587805271148682,
        "relative": 0.033674129961041535,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.33.post_attention_layernorm.weight": {
        "magnitude": 0.0019533676095306873,
        "relative": 3.679342308254526e-05,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.33.self_attn.k_proj.bias": {
        "magnitude": 0.028327826410531998,
        "relative": 0.0020914012307605696,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.33.self_attn.k_proj.weight": {
        "magnitude": 0.8494908809661865,
        "relative": 0.06447441435959797,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.33.self_attn.o_proj.weight": {
        "magnitude": 1.338515043258667,
        "relative": 0.020327291410321555,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.33.self_attn.q_proj.bias": {
        "magnitude": 0.023053746670484543,
        "relative": 0.0004366998125590888,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.33.self_attn.q_proj.weight": {
        "magnitude": 2.705589771270752,
        "relative": 0.05590774734223494,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.33.self_attn.v_proj.bias": {
        "magnitude": 0.0023081155959516764,
        "relative": 0.00038509333870650277,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.33.self_attn.v_proj.weight": {
        "magnitude": 0.30215010046958923,
        "relative": 0.008581103323601897,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.34.input_layernorm.weight": {
        "magnitude": 0.007873298600316048,
        "relative": 0.00013751275315337867,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "layernorm"
      },
      "model.layers.34.mlp.down_proj.weight": {
        "magnitude": 3.70761775970459,
        "relative": 0.028607368858551984,
        "shape": [
          2048,
          11008
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.34.mlp.gate_proj.weight": {
        "magnitude": 5.159515857696533,
        "relative": 0.042009435607354643,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.34.mlp.up_proj.weight": {
        "magnitude": 4.168696403503418,
        "relative": 0.031034814294667833,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.34.post_attention_layernorm.weight": {
        "magnitude": 0.00017674425907898694,
        "relative": 2.9031415774392977e-06,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.34.self_attn.k_proj.bias": {
        "magnitude": 0.01744399219751358,
        "relative": 0.000934450535800685,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.34.self_attn.k_proj.weight": {
        "magnitude": 0.7955568432807922,
        "relative": 0.055826719640229255,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.34.self_attn.o_proj.weight": {
        "magnitude": 1.2804063558578491,
        "relative": 0.023065245081238788,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.34.self_attn.q_proj.bias": {
        "magnitude": 0.021927736699581146,
        "relative": 0.0003863377937897241,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.34.self_attn.q_proj.weight": {
        "magnitude": 2.551741600036621,
        "relative": 0.0549939454428982,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.34.self_attn.v_proj.bias": {
        "magnitude": 0.0021383718121796846,
        "relative": 0.0002222441185592908,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.34.self_attn.v_proj.weight": {
        "magnitude": 0.4757998287677765,
        "relative": 0.017779456529481662,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.35.input_layernorm.weight": {
        "magnitude": 0.004200354218482971,
        "relative": 6.915975126735793e-05,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "layernorm"
      },
      "model.layers.35.mlp.down_proj.weight": {
        "magnitude": 3.108039379119873,
        "relative": 0.02661677214655923,
        "shape": [
          2048,
          11008
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.35.mlp.gate_proj.weight": {
        "magnitude": 4.764570713043213,
        "relative": 0.03801643147886357,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.35.mlp.up_proj.weight": {
        "magnitude": 3.8098673820495605,
        "relative": 0.029413375611613557,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.35.post_attention_layernorm.weight": {
        "magnitude": 0.0016806847415864468,
        "relative": 2.2016127760525765e-05,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.35.self_attn.k_proj.bias": {
        "magnitude": 0.009709770791232586,
        "relative": 0.0003997588832338143,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.35.self_attn.k_proj.weight": {
        "magnitude": 0.6930638551712036,
        "relative": 0.049055252979854386,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.35.self_attn.o_proj.weight": {
        "magnitude": 1.1239113807678223,
        "relative": 0.020069631441337884,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.35.self_attn.q_proj.bias": {
        "magnitude": 0.01883605495095253,
        "relative": 0.0003833187482966132,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.35.self_attn.q_proj.weight": {
        "magnitude": 2.290607213973999,
        "relative": 0.04970903497641499,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.35.self_attn.v_proj.bias": {
        "magnitude": 0.001465479377657175,
        "relative": 0.0001105883911932563,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.35.self_attn.v_proj.weight": {
        "magnitude": 0.4092137813568115,
        "relative": 0.01614455345175429,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.4.input_layernorm.weight": {
        "magnitude": 0.027733590453863144,
        "relative": 0.0012096841249369609,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "layernorm"
      },
      "model.layers.4.mlp.down_proj.weight": {
        "magnitude": 2.3210136890411377,
        "relative": 0.027747956158692803,
        "shape": [
          2048,
          11008
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.4.mlp.gate_proj.weight": {
        "magnitude": 2.5854363441467285,
        "relative": 0.029081981462022344,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.4.mlp.up_proj.weight": {
        "magnitude": 2.1281917095184326,
        "relative": 0.02505281356054075,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.4.post_attention_layernorm.weight": {
        "magnitude": 0.007325957994908094,
        "relative": 0.0001464210446318323,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.4.self_attn.k_proj.bias": {
        "magnitude": 0.017275793477892876,
        "relative": 0.0016522182846108813,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.4.self_attn.k_proj.weight": {
        "magnitude": 0.520862340927124,
        "relative": 0.020826895193328292,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.4.self_attn.o_proj.weight": {
        "magnitude": 1.756089687347412,
        "relative": 0.03669581203960118,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.4.self_attn.q_proj.bias": {
        "magnitude": 0.034215137362480164,
        "relative": 0.0005248735328057232,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.4.self_attn.q_proj.weight": {
        "magnitude": 2.0334889888763428,
        "relative": 0.03435413996670855,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.4.self_attn.v_proj.bias": {
        "magnitude": 0.005004395730793476,
        "relative": 0.007899954788718352,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.4.self_attn.v_proj.weight": {
        "magnitude": 0.43163731694221497,
        "relative": 0.027379487510421022,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.5.input_layernorm.weight": {
        "magnitude": 0.03048945777118206,
        "relative": 0.0010963157271484184,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "layernorm"
      },
      "model.layers.5.mlp.down_proj.weight": {
        "magnitude": 3.2825725078582764,
        "relative": 0.030117321452432826,
        "shape": [
          2048,
          11008
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.5.mlp.gate_proj.weight": {
        "magnitude": 3.4622204303741455,
        "relative": 0.030311999487158917,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.5.mlp.up_proj.weight": {
        "magnitude": 2.8613433837890625,
        "relative": 0.025897304997769002,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.5.post_attention_layernorm.weight": {
        "magnitude": 0.009569576941430569,
        "relative": 0.00020270185767511543,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.5.self_attn.k_proj.bias": {
        "magnitude": 0.015485536307096481,
        "relative": 0.002147494302534107,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.5.self_attn.k_proj.weight": {
        "magnitude": 0.5876665115356445,
        "relative": 0.024458671779389665,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.5.self_attn.o_proj.weight": {
        "magnitude": 1.8493670225143433,
        "relative": 0.04037336907895623,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.5.self_attn.q_proj.bias": {
        "magnitude": 0.034075092524290085,
        "relative": 0.00045415997886030687,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.5.self_attn.q_proj.weight": {
        "magnitude": 2.157844066619873,
        "relative": 0.03720113003703761,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.5.self_attn.v_proj.bias": {
        "magnitude": 0.0039430405013263226,
        "relative": 0.003539033849574446,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.5.self_attn.v_proj.weight": {
        "magnitude": 0.5141874551773071,
        "relative": 0.029797336339470536,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.6.input_layernorm.weight": {
        "magnitude": 0.02467275597155094,
        "relative": 0.0009718428630063728,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "layernorm"
      },
      "model.layers.6.mlp.down_proj.weight": {
        "magnitude": 3.848375082015991,
        "relative": 0.03214020038938458,
        "shape": [
          2048,
          11008
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.6.mlp.gate_proj.weight": {
        "magnitude": 4.180819988250732,
        "relative": 0.03335591016846428,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.6.mlp.up_proj.weight": {
        "magnitude": 3.3971519470214844,
        "relative": 0.028104719342760827,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.6.post_attention_layernorm.weight": {
        "magnitude": 0.009164899587631226,
        "relative": 0.00018379971366679873,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.6.self_attn.k_proj.bias": {
        "magnitude": 0.017410606145858765,
        "relative": 0.0019970344475458055,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.6.self_attn.k_proj.weight": {
        "magnitude": 0.710971474647522,
        "relative": 0.030212607232714687,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.6.self_attn.o_proj.weight": {
        "magnitude": 1.8183375597000122,
        "relative": 0.03588850520284015,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.6.self_attn.q_proj.bias": {
        "magnitude": 0.02931007370352745,
        "relative": 0.00045839077301045115,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.6.self_attn.q_proj.weight": {
        "magnitude": 2.2651493549346924,
        "relative": 0.03875066506718852,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.6.self_attn.v_proj.bias": {
        "magnitude": 0.005781278945505619,
        "relative": 0.01296620354955026,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.6.self_attn.v_proj.weight": {
        "magnitude": 0.5352602601051331,
        "relative": 0.033047976894456224,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.7.input_layernorm.weight": {
        "magnitude": 0.04925764724612236,
        "relative": 0.0015985776727442194,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "layernorm"
      },
      "model.layers.7.mlp.down_proj.weight": {
        "magnitude": 4.074197292327881,
        "relative": 0.03537879936420622,
        "shape": [
          2048,
          11008
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.7.mlp.gate_proj.weight": {
        "magnitude": 4.568172931671143,
        "relative": 0.03663096450875569,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.7.mlp.up_proj.weight": {
        "magnitude": 3.686018705368042,
        "relative": 0.03177089546012209,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.7.post_attention_layernorm.weight": {
        "magnitude": 0.009569956921041012,
        "relative": 0.00018185946663298053,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.7.self_attn.k_proj.bias": {
        "magnitude": 0.020826704800128937,
        "relative": 0.002473671994731693,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.7.self_attn.k_proj.weight": {
        "magnitude": 1.0052343606948853,
        "relative": 0.051394771729590685,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.7.self_attn.o_proj.weight": {
        "magnitude": 2.4929800033569336,
        "relative": 0.049779241249382856,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.7.self_attn.q_proj.bias": {
        "magnitude": 0.04569140449166298,
        "relative": 0.0008109346330340283,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.7.self_attn.q_proj.weight": {
        "magnitude": 2.980534076690674,
        "relative": 0.05421939061410571,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.7.self_attn.v_proj.bias": {
        "magnitude": 0.004570184741169214,
        "relative": 0.01031773102577684,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.7.self_attn.v_proj.weight": {
        "magnitude": 0.7678385376930237,
        "relative": 0.04298225471663545,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.8.input_layernorm.weight": {
        "magnitude": 0.05699481815099716,
        "relative": 0.0018148046851143825,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "layernorm"
      },
      "model.layers.8.mlp.down_proj.weight": {
        "magnitude": 4.691233158111572,
        "relative": 0.03930117175955518,
        "shape": [
          2048,
          11008
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.8.mlp.gate_proj.weight": {
        "magnitude": 5.375040054321289,
        "relative": 0.03796359172965973,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.8.mlp.up_proj.weight": {
        "magnitude": 4.292032718658447,
        "relative": 0.035588857461022735,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.8.post_attention_layernorm.weight": {
        "magnitude": 0.014875504188239574,
        "relative": 0.0002868311848529626,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.8.self_attn.k_proj.bias": {
        "magnitude": 0.02035890705883503,
        "relative": 0.00254444054976919,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.8.self_attn.k_proj.weight": {
        "magnitude": 1.0603116750717163,
        "relative": 0.04881582743019063,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.8.self_attn.o_proj.weight": {
        "magnitude": 2.712367296218872,
        "relative": 0.05310953395533603,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.8.self_attn.q_proj.bias": {
        "magnitude": 0.04659741744399071,
        "relative": 0.0007880467810450263,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.8.self_attn.q_proj.weight": {
        "magnitude": 3.095555305480957,
        "relative": 0.05544107450186356,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.8.self_attn.v_proj.bias": {
        "magnitude": 0.004598247352987528,
        "relative": 0.010016480039046736,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.8.self_attn.v_proj.weight": {
        "magnitude": 0.828515887260437,
        "relative": 0.04603957364663734,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.9.input_layernorm.weight": {
        "magnitude": 0.06326231360435486,
        "relative": 0.002299579956637262,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "layernorm"
      },
      "model.layers.9.mlp.down_proj.weight": {
        "magnitude": 4.677149772644043,
        "relative": 0.04020228087190847,
        "shape": [
          2048,
          11008
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.9.mlp.gate_proj.weight": {
        "magnitude": 5.725837230682373,
        "relative": 0.03996231213497277,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.9.mlp.up_proj.weight": {
        "magnitude": 4.416999816894531,
        "relative": 0.03727916115536053,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.9.post_attention_layernorm.weight": {
        "magnitude": 0.01832246221601963,
        "relative": 0.0003727090802399753,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.9.self_attn.k_proj.bias": {
        "magnitude": 0.019355598837137222,
        "relative": 0.002191838866772217,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.9.self_attn.k_proj.weight": {
        "magnitude": 0.9881808161735535,
        "relative": 0.041320285619053196,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.9.self_attn.o_proj.weight": {
        "magnitude": 2.6949310302734375,
        "relative": 0.05254497962584194,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.9.self_attn.q_proj.bias": {
        "magnitude": 0.04150714725255966,
        "relative": 0.0006214460038000008,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.9.self_attn.q_proj.weight": {
        "magnitude": 3.1272168159484863,
        "relative": 0.054214918417358725,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.9.self_attn.v_proj.bias": {
        "magnitude": 0.004965526517480612,
        "relative": 0.0075112649239727335,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.9.self_attn.v_proj.weight": {
        "magnitude": 0.7728608250617981,
        "relative": 0.04139856448438128,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.norm.weight": {
        "magnitude": 0.0008802615338936448,
        "relative": 6.053548279927785e-06,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "other"
      }
    },
    "sft": {
      "lm_head.weight": {
        "magnitude": 30.050071716308594,
        "relative": 0.07393166949230573,
        "shape": [
          151936,
          2048
        ],
        "param_count": 311164928,
        "layer_type": "output"
      },
      "model.embed_tokens.weight": {
        "magnitude": 30.050071716308594,
        "relative": 0.07393166949230573,
        "shape": [
          151936,
          2048
        ],
        "param_count": 311164928,
        "layer_type": "embedding"
      },
      "model.layers.0.input_layernorm.weight": {
        "magnitude": 0.013354291208088398,
        "relative": 0.0008422813185060433,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "layernorm"
      },
      "model.layers.0.mlp.down_proj.weight": {
        "magnitude": 2.551006317138672,
        "relative": 0.0212585035617794,
        "shape": [
          2048,
          11008
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.0.mlp.gate_proj.weight": {
        "magnitude": 2.344773054122925,
        "relative": 0.01843665143602071,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.0.mlp.up_proj.weight": {
        "magnitude": 2.273460865020752,
        "relative": 0.02032596781545828,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.0.post_attention_layernorm.weight": {
        "magnitude": 0.041695237159729004,
        "relative": 0.002235218574695419,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.0.self_attn.k_proj.bias": {
        "magnitude": 0.04801356792449951,
        "relative": 0.00019561608674423844,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.0.self_attn.k_proj.weight": {
        "magnitude": 0.42919251322746277,
        "relative": 0.011611798139170819,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.0.self_attn.o_proj.weight": {
        "magnitude": 0.8720042705535889,
        "relative": 0.019742169155848588,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.0.self_attn.q_proj.bias": {
        "magnitude": 0.03312576934695244,
        "relative": 0.0005573287308527792,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.0.self_attn.q_proj.weight": {
        "magnitude": 1.3042404651641846,
        "relative": 0.017406047614760986,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.0.self_attn.v_proj.bias": {
        "magnitude": 0.004616131074726582,
        "relative": 0.001752327742243302,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.0.self_attn.v_proj.weight": {
        "magnitude": 0.24544714391231537,
        "relative": 0.020526919373934076,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.1.input_layernorm.weight": {
        "magnitude": 0.030992703512310982,
        "relative": 0.002478213020666163,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "layernorm"
      },
      "model.layers.1.mlp.down_proj.weight": {
        "magnitude": 1.1181418895721436,
        "relative": 0.02550772025304009,
        "shape": [
          2048,
          11008
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.1.mlp.gate_proj.weight": {
        "magnitude": 1.334533929824829,
        "relative": 0.023446045483640456,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.1.mlp.up_proj.weight": {
        "magnitude": 0.8772315979003906,
        "relative": 0.01951153330991425,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.1.post_attention_layernorm.weight": {
        "magnitude": 0.009519808925688267,
        "relative": 0.0001618099757580423,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.1.self_attn.k_proj.bias": {
        "magnitude": 0.019494352862238884,
        "relative": 0.0002699013405373122,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.1.self_attn.k_proj.weight": {
        "magnitude": 0.618877112865448,
        "relative": 0.02018080721054764,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.1.self_attn.o_proj.weight": {
        "magnitude": 1.335896372795105,
        "relative": 0.02926103588677259,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.1.self_attn.q_proj.bias": {
        "magnitude": 0.04241471737623215,
        "relative": 0.0012503287727915005,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.1.self_attn.q_proj.weight": {
        "magnitude": 2.0880377292633057,
        "relative": 0.03531276885583702,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.1.self_attn.v_proj.bias": {
        "magnitude": 0.0048717837780714035,
        "relative": 0.002013472532420415,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.1.self_attn.v_proj.weight": {
        "magnitude": 0.43575674295425415,
        "relative": 0.03262220776088726,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.10.input_layernorm.weight": {
        "magnitude": 0.04456159099936485,
        "relative": 0.0015192642519805313,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "layernorm"
      },
      "model.layers.10.mlp.down_proj.weight": {
        "magnitude": 4.698995590209961,
        "relative": 0.04073150777055354,
        "shape": [
          2048,
          11008
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.10.mlp.gate_proj.weight": {
        "magnitude": 6.205859661102295,
        "relative": 0.042729205626160546,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.10.mlp.up_proj.weight": {
        "magnitude": 4.446094036102295,
        "relative": 0.03757942831285312,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.10.post_attention_layernorm.weight": {
        "magnitude": 0.039451442658901215,
        "relative": 0.0009168478131179384,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.10.self_attn.k_proj.bias": {
        "magnitude": 0.016534509137272835,
        "relative": 0.0016539818514531835,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.10.self_attn.k_proj.weight": {
        "magnitude": 0.8964884877204895,
        "relative": 0.04502000784409479,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.10.self_attn.o_proj.weight": {
        "magnitude": 2.3015830516815186,
        "relative": 0.044882572387774114,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.10.self_attn.q_proj.bias": {
        "magnitude": 0.04042060300707817,
        "relative": 0.0006340205998308715,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.10.self_attn.q_proj.weight": {
        "magnitude": 2.766111373901367,
        "relative": 0.051062728982375,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.10.self_attn.v_proj.bias": {
        "magnitude": 0.005351764149963856,
        "relative": 0.005409645949669425,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.10.self_attn.v_proj.weight": {
        "magnitude": 0.8230741024017334,
        "relative": 0.04211693411835782,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.11.input_layernorm.weight": {
        "magnitude": 0.06534594297409058,
        "relative": 0.002234125670091539,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "layernorm"
      },
      "model.layers.11.mlp.down_proj.weight": {
        "magnitude": 5.285706996917725,
        "relative": 0.04451375926280065,
        "shape": [
          2048,
          11008
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.11.mlp.gate_proj.weight": {
        "magnitude": 6.40435791015625,
        "relative": 0.04547535810291853,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.11.mlp.up_proj.weight": {
        "magnitude": 5.020083904266357,
        "relative": 0.04138043429112501,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.11.post_attention_layernorm.weight": {
        "magnitude": 0.05101563408970833,
        "relative": 0.001488622690422045,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.11.self_attn.k_proj.bias": {
        "magnitude": 0.01851607859134674,
        "relative": 0.0016599115467694537,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.11.self_attn.k_proj.weight": {
        "magnitude": 1.0690553188323975,
        "relative": 0.05255998264836796,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.11.self_attn.o_proj.weight": {
        "magnitude": 2.716733932495117,
        "relative": 0.05119545351665949,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.11.self_attn.q_proj.bias": {
        "magnitude": 0.04661761224269867,
        "relative": 0.0006982572742994794,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.11.self_attn.q_proj.weight": {
        "magnitude": 3.2702934741973877,
        "relative": 0.05911294244629141,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.11.self_attn.v_proj.bias": {
        "magnitude": 0.007254861295223236,
        "relative": 0.0065836663251469625,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.11.self_attn.v_proj.weight": {
        "magnitude": 0.9074947834014893,
        "relative": 0.04610837867399282,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.12.input_layernorm.weight": {
        "magnitude": 0.06568985432386398,
        "relative": 0.002080821091403319,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "layernorm"
      },
      "model.layers.12.mlp.down_proj.weight": {
        "magnitude": 5.554587364196777,
        "relative": 0.04715730772470102,
        "shape": [
          2048,
          11008
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.12.mlp.gate_proj.weight": {
        "magnitude": 7.241189956665039,
        "relative": 0.050520007014109594,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.12.mlp.up_proj.weight": {
        "magnitude": 5.283947467803955,
        "relative": 0.0437785350316503,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.12.post_attention_layernorm.weight": {
        "magnitude": 0.05383508652448654,
        "relative": 0.0014156483812621577,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.12.self_attn.k_proj.bias": {
        "magnitude": 0.01549450121819973,
        "relative": 0.0013438880828981596,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.12.self_attn.k_proj.weight": {
        "magnitude": 1.0704094171524048,
        "relative": 0.05014093465353048,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.12.self_attn.o_proj.weight": {
        "magnitude": 2.6651225090026855,
        "relative": 0.05036519758845199,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.12.self_attn.q_proj.bias": {
        "magnitude": 0.05072591081261635,
        "relative": 0.000856756720514414,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.12.self_attn.q_proj.weight": {
        "magnitude": 3.361734390258789,
        "relative": 0.06079366335326632,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.12.self_attn.v_proj.bias": {
        "magnitude": 0.006752536632120609,
        "relative": 0.003961803896938386,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.12.self_attn.v_proj.weight": {
        "magnitude": 1.0019433498382568,
        "relative": 0.05274282968189408,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.13.input_layernorm.weight": {
        "magnitude": 0.05354714021086693,
        "relative": 0.0019443045842684635,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "layernorm"
      },
      "model.layers.13.mlp.down_proj.weight": {
        "magnitude": 6.325946807861328,
        "relative": 0.05036405087036048,
        "shape": [
          2048,
          11008
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.13.mlp.gate_proj.weight": {
        "magnitude": 7.185274600982666,
        "relative": 0.055131232015553214,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.13.mlp.up_proj.weight": {
        "magnitude": 5.931147575378418,
        "relative": 0.04635773934873289,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.13.post_attention_layernorm.weight": {
        "magnitude": 0.060853831470012665,
        "relative": 0.001792377359918032,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.13.self_attn.k_proj.bias": {
        "magnitude": 0.015066251158714294,
        "relative": 0.001453709755463819,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.13.self_attn.k_proj.weight": {
        "magnitude": 1.0263087749481201,
        "relative": 0.04100612711005849,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.13.self_attn.o_proj.weight": {
        "magnitude": 2.4012324810028076,
        "relative": 0.05045072389111258,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.13.self_attn.q_proj.bias": {
        "magnitude": 0.036629270762205124,
        "relative": 0.0005163709822847857,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.13.self_attn.q_proj.weight": {
        "magnitude": 3.239387273788452,
        "relative": 0.055627539897743446,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.13.self_attn.v_proj.bias": {
        "magnitude": 0.00705289700999856,
        "relative": 0.0053747409860498765,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.13.self_attn.v_proj.weight": {
        "magnitude": 0.8855698108673096,
        "relative": 0.05590813368030011,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.14.input_layernorm.weight": {
        "magnitude": 0.07330195605754852,
        "relative": 0.0024293609273832906,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "layernorm"
      },
      "model.layers.14.mlp.down_proj.weight": {
        "magnitude": 6.887596607208252,
        "relative": 0.05527909381676932,
        "shape": [
          2048,
          11008
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.14.mlp.gate_proj.weight": {
        "magnitude": 8.224862098693848,
        "relative": 0.06317435402872251,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.14.mlp.up_proj.weight": {
        "magnitude": 6.523976802825928,
        "relative": 0.0511027738839594,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.14.post_attention_layernorm.weight": {
        "magnitude": 0.06496509909629822,
        "relative": 0.0018588966403644978,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.14.self_attn.k_proj.bias": {
        "magnitude": 0.02597607485949993,
        "relative": 0.0022827702145742724,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.14.self_attn.k_proj.weight": {
        "magnitude": 1.222292184829712,
        "relative": 0.054863233381110044,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.14.self_attn.o_proj.weight": {
        "magnitude": 2.909846782684326,
        "relative": 0.05710174634812243,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.14.self_attn.q_proj.bias": {
        "magnitude": 0.04750072956085205,
        "relative": 0.0007192826443352905,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.14.self_attn.q_proj.weight": {
        "magnitude": 3.8644471168518066,
        "relative": 0.06677608517885272,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.14.self_attn.v_proj.bias": {
        "magnitude": 0.008688034489750862,
        "relative": 0.008764152044193915,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.14.self_attn.v_proj.weight": {
        "magnitude": 0.9452444911003113,
        "relative": 0.051075165970698944,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.15.input_layernorm.weight": {
        "magnitude": 0.0646900087594986,
        "relative": 0.0019680225051780637,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "layernorm"
      },
      "model.layers.15.mlp.down_proj.weight": {
        "magnitude": 7.323359489440918,
        "relative": 0.058088830841082253,
        "shape": [
          2048,
          11008
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.15.mlp.gate_proj.weight": {
        "magnitude": 8.733877182006836,
        "relative": 0.06930494947906027,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.15.mlp.up_proj.weight": {
        "magnitude": 7.01692533493042,
        "relative": 0.054207751986750735,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.15.post_attention_layernorm.weight": {
        "magnitude": 0.0730026438832283,
        "relative": 0.0020663109594296373,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.15.self_attn.k_proj.bias": {
        "magnitude": 0.0232352614402771,
        "relative": 0.002057088964971888,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.15.self_attn.k_proj.weight": {
        "magnitude": 1.1682931184768677,
        "relative": 0.05311813118166614,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.15.self_attn.o_proj.weight": {
        "magnitude": 2.7744882106781006,
        "relative": 0.05579794876530798,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.15.self_attn.q_proj.bias": {
        "magnitude": 0.038098860532045364,
        "relative": 0.0005646570676146654,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.15.self_attn.q_proj.weight": {
        "magnitude": 3.6470565795898438,
        "relative": 0.06368046573055888,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.15.self_attn.v_proj.bias": {
        "magnitude": 0.008696905337274075,
        "relative": 0.007126894809221446,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.15.self_attn.v_proj.weight": {
        "magnitude": 0.9903433918952942,
        "relative": 0.05598175246703408,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.16.input_layernorm.weight": {
        "magnitude": 0.08176969736814499,
        "relative": 0.002161600483761955,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "layernorm"
      },
      "model.layers.16.mlp.down_proj.weight": {
        "magnitude": 8.012750625610352,
        "relative": 0.06568363600506529,
        "shape": [
          2048,
          11008
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.16.mlp.gate_proj.weight": {
        "magnitude": 9.610475540161133,
        "relative": 0.07496536985463562,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.16.mlp.up_proj.weight": {
        "magnitude": 7.694960117340088,
        "relative": 0.061174754918563994,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.16.post_attention_layernorm.weight": {
        "magnitude": 0.08425646275281906,
        "relative": 0.0023412377781113014,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.16.self_attn.k_proj.bias": {
        "magnitude": 0.0339527353644371,
        "relative": 0.0020312729455869197,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.16.self_attn.k_proj.weight": {
        "magnitude": 1.4723460674285889,
        "relative": 0.06573350314154641,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.16.self_attn.o_proj.weight": {
        "magnitude": 3.4000403881073,
        "relative": 0.0665441442864259,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.16.self_attn.q_proj.bias": {
        "magnitude": 0.05011945590376854,
        "relative": 0.0008687884946015303,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.16.self_attn.q_proj.weight": {
        "magnitude": 4.40485143661499,
        "relative": 0.07594865158396993,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.16.self_attn.v_proj.bias": {
        "magnitude": 0.006920890882611275,
        "relative": 0.010222248122811356,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.16.self_attn.v_proj.weight": {
        "magnitude": 1.0853618383407593,
        "relative": 0.05940873038845806,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.17.input_layernorm.weight": {
        "magnitude": 0.06736864894628525,
        "relative": 0.0016275242786188296,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "layernorm"
      },
      "model.layers.17.mlp.down_proj.weight": {
        "magnitude": 8.111441612243652,
        "relative": 0.06662669804075193,
        "shape": [
          2048,
          11008
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.17.mlp.gate_proj.weight": {
        "magnitude": 9.550272941589355,
        "relative": 0.07672794949476232,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.17.mlp.up_proj.weight": {
        "magnitude": 7.984694480895996,
        "relative": 0.0634838890840836,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.17.post_attention_layernorm.weight": {
        "magnitude": 0.0890762060880661,
        "relative": 0.0024435149131853836,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.17.self_attn.k_proj.bias": {
        "magnitude": 0.02493993565440178,
        "relative": 0.002994025889804911,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.17.self_attn.k_proj.weight": {
        "magnitude": 1.225140929222107,
        "relative": 0.06041472398712455,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.17.self_attn.o_proj.weight": {
        "magnitude": 2.542804002761841,
        "relative": 0.052434062415049595,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.17.self_attn.q_proj.bias": {
        "magnitude": 0.041996341198682785,
        "relative": 0.0007098166132966398,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.17.self_attn.q_proj.weight": {
        "magnitude": 3.6613245010375977,
        "relative": 0.06731370758965836,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.17.self_attn.v_proj.bias": {
        "magnitude": 0.007589736487716436,
        "relative": 0.0055633316514242205,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.17.self_attn.v_proj.weight": {
        "magnitude": 0.9390820264816284,
        "relative": 0.05543055548374535,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.18.input_layernorm.weight": {
        "magnitude": 0.06786233931779861,
        "relative": 0.001576772180738809,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "layernorm"
      },
      "model.layers.18.mlp.down_proj.weight": {
        "magnitude": 8.249228477478027,
        "relative": 0.06870766745193202,
        "shape": [
          2048,
          11008
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.18.mlp.gate_proj.weight": {
        "magnitude": 10.070116996765137,
        "relative": 0.08022662734169592,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.18.mlp.up_proj.weight": {
        "magnitude": 8.2390718460083,
        "relative": 0.06641868481674654,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.18.post_attention_layernorm.weight": {
        "magnitude": 0.08489637821912766,
        "relative": 0.0022815198413012965,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.18.self_attn.k_proj.bias": {
        "magnitude": 0.033411793410778046,
        "relative": 0.0008858523440947103,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.18.self_attn.k_proj.weight": {
        "magnitude": 1.372942328453064,
        "relative": 0.06623611951669325,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.18.self_attn.o_proj.weight": {
        "magnitude": 2.795917272567749,
        "relative": 0.056962835021520816,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.18.self_attn.q_proj.bias": {
        "magnitude": 0.046957116574048996,
        "relative": 0.0007369457616538083,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.18.self_attn.q_proj.weight": {
        "magnitude": 4.172228813171387,
        "relative": 0.0753911867444632,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.18.self_attn.v_proj.bias": {
        "magnitude": 0.006202542223036289,
        "relative": 0.008709128625387991,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.18.self_attn.v_proj.weight": {
        "magnitude": 0.9899297952651978,
        "relative": 0.05458552186013275,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.19.input_layernorm.weight": {
        "magnitude": 0.0939047634601593,
        "relative": 0.0019867223188337756,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "layernorm"
      },
      "model.layers.19.mlp.down_proj.weight": {
        "magnitude": 8.538045883178711,
        "relative": 0.07036031044755126,
        "shape": [
          2048,
          11008
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.19.mlp.gate_proj.weight": {
        "magnitude": 10.034940719604492,
        "relative": 0.08358193025368087,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.19.mlp.up_proj.weight": {
        "magnitude": 8.503364562988281,
        "relative": 0.06934853863283437,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.19.post_attention_layernorm.weight": {
        "magnitude": 0.07345684617757797,
        "relative": 0.0018965326305089604,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.19.self_attn.k_proj.bias": {
        "magnitude": 0.024799639359116554,
        "relative": 0.0009394298385168759,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.19.self_attn.k_proj.weight": {
        "magnitude": 1.2495126724243164,
        "relative": 0.05577642141479492,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.19.self_attn.o_proj.weight": {
        "magnitude": 2.6122076511383057,
        "relative": 0.055248157783496,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.19.self_attn.q_proj.bias": {
        "magnitude": 0.03535653278231621,
        "relative": 0.0005628282398995076,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.19.self_attn.q_proj.weight": {
        "magnitude": 3.94545578956604,
        "relative": 0.07120552069991658,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.19.self_attn.v_proj.bias": {
        "magnitude": 0.008990924805402756,
        "relative": 0.010772967025594628,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.19.self_attn.v_proj.weight": {
        "magnitude": 0.9838135838508606,
        "relative": 0.06115052190930275,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.2.input_layernorm.weight": {
        "magnitude": 0.024994079023599625,
        "relative": 0.001405284695769695,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "layernorm"
      },
      "model.layers.2.mlp.down_proj.weight": {
        "magnitude": 1.4091168642044067,
        "relative": 0.02403181453122783,
        "shape": [
          2048,
          11008
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.2.mlp.gate_proj.weight": {
        "magnitude": 1.5701451301574707,
        "relative": 0.02765399056505195,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.2.mlp.up_proj.weight": {
        "magnitude": 1.3158355951309204,
        "relative": 0.019827394200125498,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.2.post_attention_layernorm.weight": {
        "magnitude": 0.0025244499556720257,
        "relative": 4.1841017085302955e-05,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.2.self_attn.k_proj.bias": {
        "magnitude": 0.026458336040377617,
        "relative": 0.0005636084003874204,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.2.self_attn.k_proj.weight": {
        "magnitude": 0.6217042803764343,
        "relative": 0.022959520897476306,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.2.self_attn.o_proj.weight": {
        "magnitude": 1.3397202491760254,
        "relative": 0.029316911470357997,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.2.self_attn.q_proj.bias": {
        "magnitude": 0.048084598034620285,
        "relative": 0.001429933563683439,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.2.self_attn.q_proj.weight": {
        "magnitude": 2.2469708919525146,
        "relative": 0.038899092587618284,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.2.self_attn.v_proj.bias": {
        "magnitude": 0.004212712869048119,
        "relative": 0.01145487860311039,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.2.self_attn.v_proj.weight": {
        "magnitude": 0.43394023180007935,
        "relative": 0.031708430855907535,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.20.input_layernorm.weight": {
        "magnitude": 0.06134992837905884,
        "relative": 0.0010644448767147753,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "layernorm"
      },
      "model.layers.20.mlp.down_proj.weight": {
        "magnitude": 8.846943855285645,
        "relative": 0.07186801573663683,
        "shape": [
          2048,
          11008
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.20.mlp.gate_proj.weight": {
        "magnitude": 10.507711410522461,
        "relative": 0.0844288102532593,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.20.mlp.up_proj.weight": {
        "magnitude": 8.866968154907227,
        "relative": 0.06969775084552507,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.20.post_attention_layernorm.weight": {
        "magnitude": 0.09163572639226913,
        "relative": 0.002411080912656346,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.20.self_attn.k_proj.bias": {
        "magnitude": 0.03112313523888588,
        "relative": 0.0010199100817502437,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.20.self_attn.k_proj.weight": {
        "magnitude": 1.328198790550232,
        "relative": 0.0724010607031585,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.20.self_attn.o_proj.weight": {
        "magnitude": 2.8766062259674072,
        "relative": 0.05518518375839808,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.20.self_attn.q_proj.bias": {
        "magnitude": 0.04652038589119911,
        "relative": 0.0007294514463667897,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.20.self_attn.q_proj.weight": {
        "magnitude": 4.140956401824951,
        "relative": 0.07794983262891737,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.20.self_attn.v_proj.bias": {
        "magnitude": 0.004638201557099819,
        "relative": 0.0035767891079493434,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.20.self_attn.v_proj.weight": {
        "magnitude": 0.9459095597267151,
        "relative": 0.05343782400318344,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.21.input_layernorm.weight": {
        "magnitude": 0.09061434864997864,
        "relative": 0.0015807328603839623,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "layernorm"
      },
      "model.layers.21.mlp.down_proj.weight": {
        "magnitude": 9.001230239868164,
        "relative": 0.07443748340832125,
        "shape": [
          2048,
          11008
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.21.mlp.gate_proj.weight": {
        "magnitude": 10.914824485778809,
        "relative": 0.0856327943201986,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.21.mlp.up_proj.weight": {
        "magnitude": 9.02408218383789,
        "relative": 0.07208401667978785,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.21.post_attention_layernorm.weight": {
        "magnitude": 0.10294321179389954,
        "relative": 0.0026425549252788515,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.21.self_attn.k_proj.bias": {
        "magnitude": 0.034017618745565414,
        "relative": 0.001540136024728512,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.21.self_attn.k_proj.weight": {
        "magnitude": 1.494519591331482,
        "relative": 0.07440485908622334,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.21.self_attn.o_proj.weight": {
        "magnitude": 3.0171096324920654,
        "relative": 0.059617554975343275,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.21.self_attn.q_proj.bias": {
        "magnitude": 0.048041049391031265,
        "relative": 0.0009909560400327658,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.21.self_attn.q_proj.weight": {
        "magnitude": 4.334500312805176,
        "relative": 0.08028408681542867,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.21.self_attn.v_proj.bias": {
        "magnitude": 0.006149796769022942,
        "relative": 0.008438503411851183,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.21.self_attn.v_proj.weight": {
        "magnitude": 1.1240710020065308,
        "relative": 0.062036675153647636,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.22.input_layernorm.weight": {
        "magnitude": 0.11220964789390564,
        "relative": 0.002193605409591102,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "layernorm"
      },
      "model.layers.22.mlp.down_proj.weight": {
        "magnitude": 9.180123329162598,
        "relative": 0.07572400244409434,
        "shape": [
          2048,
          11008
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.22.mlp.gate_proj.weight": {
        "magnitude": 11.006889343261719,
        "relative": 0.08731872011542002,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.22.mlp.up_proj.weight": {
        "magnitude": 9.08497142791748,
        "relative": 0.07236064524910832,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.22.post_attention_layernorm.weight": {
        "magnitude": 0.09954260289669037,
        "relative": 0.0025147496686439147,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.22.self_attn.k_proj.bias": {
        "magnitude": 0.026582984253764153,
        "relative": 0.0019580412194907127,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.22.self_attn.k_proj.weight": {
        "magnitude": 1.4363200664520264,
        "relative": 0.0760310214734185,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.22.self_attn.o_proj.weight": {
        "magnitude": 2.9909791946411133,
        "relative": 0.05797106618338168,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.22.self_attn.q_proj.bias": {
        "magnitude": 0.04257518798112869,
        "relative": 0.000777394145465476,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.22.self_attn.q_proj.weight": {
        "magnitude": 4.291072845458984,
        "relative": 0.07899567090816643,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.22.self_attn.v_proj.bias": {
        "magnitude": 0.008511866442859173,
        "relative": 0.009324803441228418,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.22.self_attn.v_proj.weight": {
        "magnitude": 1.0876564979553223,
        "relative": 0.057210631153146334,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.23.input_layernorm.weight": {
        "magnitude": 0.0963079035282135,
        "relative": 0.002025579642483926,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "layernorm"
      },
      "model.layers.23.mlp.down_proj.weight": {
        "magnitude": 8.87942123413086,
        "relative": 0.07303725211872714,
        "shape": [
          2048,
          11008
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.23.mlp.gate_proj.weight": {
        "magnitude": 10.61266040802002,
        "relative": 0.08702555343291042,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.23.mlp.up_proj.weight": {
        "magnitude": 9.08806324005127,
        "relative": 0.07178024327896541,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.23.post_attention_layernorm.weight": {
        "magnitude": 0.09188602864742279,
        "relative": 0.002311080551213209,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.23.self_attn.k_proj.bias": {
        "magnitude": 0.03794502094388008,
        "relative": 0.0019453409910272549,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.23.self_attn.k_proj.weight": {
        "magnitude": 1.4281952381134033,
        "relative": 0.06552541264444729,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.23.self_attn.o_proj.weight": {
        "magnitude": 2.781898021697998,
        "relative": 0.05885103322864342,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.23.self_attn.q_proj.bias": {
        "magnitude": 0.031701575964689255,
        "relative": 0.0004869644164392972,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.23.self_attn.q_proj.weight": {
        "magnitude": 4.119295597076416,
        "relative": 0.07478286381952595,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.23.self_attn.v_proj.bias": {
        "magnitude": 0.008350774645805359,
        "relative": 0.008258517814025049,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.23.self_attn.v_proj.weight": {
        "magnitude": 1.1127982139587402,
        "relative": 0.06558062491133068,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.24.input_layernorm.weight": {
        "magnitude": 0.0907500609755516,
        "relative": 0.001956112100359858,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "layernorm"
      },
      "model.layers.24.mlp.down_proj.weight": {
        "magnitude": 8.808438301086426,
        "relative": 0.0729791167361158,
        "shape": [
          2048,
          11008
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.24.mlp.gate_proj.weight": {
        "magnitude": 10.57171630859375,
        "relative": 0.08690141433523742,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.24.mlp.up_proj.weight": {
        "magnitude": 8.902600288391113,
        "relative": 0.07113059963611622,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.24.post_attention_layernorm.weight": {
        "magnitude": 0.09651680290699005,
        "relative": 0.0024724241913359267,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.24.self_attn.k_proj.bias": {
        "magnitude": 0.038778532296419144,
        "relative": 0.0028189029588665706,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.24.self_attn.k_proj.weight": {
        "magnitude": 1.5443211793899536,
        "relative": 0.07818066459388176,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.24.self_attn.o_proj.weight": {
        "magnitude": 3.3127965927124023,
        "relative": 0.06584995625020687,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.24.self_attn.q_proj.bias": {
        "magnitude": 0.04254649206995964,
        "relative": 0.0005787336587707765,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.24.self_attn.q_proj.weight": {
        "magnitude": 4.640129089355469,
        "relative": 0.08481526919015084,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.24.self_attn.v_proj.bias": {
        "magnitude": 0.006496315822005272,
        "relative": 0.00916324349562124,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.24.self_attn.v_proj.weight": {
        "magnitude": 1.0809526443481445,
        "relative": 0.0577808710542552,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.25.input_layernorm.weight": {
        "magnitude": 0.04928983002901077,
        "relative": 0.0009035094269513703,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "layernorm"
      },
      "model.layers.25.mlp.down_proj.weight": {
        "magnitude": 8.424965858459473,
        "relative": 0.0687311581299953,
        "shape": [
          2048,
          11008
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.25.mlp.gate_proj.weight": {
        "magnitude": 10.428874015808105,
        "relative": 0.08484487836821208,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.25.mlp.up_proj.weight": {
        "magnitude": 8.582745552062988,
        "relative": 0.06750495729544673,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.25.post_attention_layernorm.weight": {
        "magnitude": 0.08504508435726166,
        "relative": 0.0021940368871603776,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.25.self_attn.k_proj.bias": {
        "magnitude": 0.029576072469353676,
        "relative": 0.0020020778016980375,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.25.self_attn.k_proj.weight": {
        "magnitude": 1.4478474855422974,
        "relative": 0.09356163333645796,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.25.self_attn.o_proj.weight": {
        "magnitude": 2.7548413276672363,
        "relative": 0.05294256128138727,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.25.self_attn.q_proj.bias": {
        "magnitude": 0.04406798630952835,
        "relative": 0.0007432231006671149,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.25.self_attn.q_proj.weight": {
        "magnitude": 4.363818645477295,
        "relative": 0.08346754829905707,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.25.self_attn.v_proj.bias": {
        "magnitude": 0.004270609933882952,
        "relative": 0.004736735409586068,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.25.self_attn.v_proj.weight": {
        "magnitude": 0.9327594637870789,
        "relative": 0.04964806615067814,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.26.input_layernorm.weight": {
        "magnitude": 0.07597638666629791,
        "relative": 0.0015922391195909741,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "layernorm"
      },
      "model.layers.26.mlp.down_proj.weight": {
        "magnitude": 7.940762042999268,
        "relative": 0.06361756211607798,
        "shape": [
          2048,
          11008
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.26.mlp.gate_proj.weight": {
        "magnitude": 10.452475547790527,
        "relative": 0.08556431399113029,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.26.mlp.up_proj.weight": {
        "magnitude": 8.182351112365723,
        "relative": 0.06295003632206465,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.26.post_attention_layernorm.weight": {
        "magnitude": 0.0841667503118515,
        "relative": 0.0021573248029184665,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.26.self_attn.k_proj.bias": {
        "magnitude": 0.02188066951930523,
        "relative": 0.0016377341390755463,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.26.self_attn.k_proj.weight": {
        "magnitude": 1.4864121675491333,
        "relative": 0.07870821512029892,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.26.self_attn.o_proj.weight": {
        "magnitude": 3.209085702896118,
        "relative": 0.06073701799945486,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.26.self_attn.q_proj.bias": {
        "magnitude": 0.03574502840638161,
        "relative": 0.0005180907496394706,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.26.self_attn.q_proj.weight": {
        "magnitude": 4.3181023597717285,
        "relative": 0.08033557365326757,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.26.self_attn.v_proj.bias": {
        "magnitude": 0.005014638416469097,
        "relative": 0.006534997645149088,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.26.self_attn.v_proj.weight": {
        "magnitude": 1.0353920459747314,
        "relative": 0.04946840092945761,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.27.input_layernorm.weight": {
        "magnitude": 0.0402873195707798,
        "relative": 0.0007907465673979006,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "layernorm"
      },
      "model.layers.27.mlp.down_proj.weight": {
        "magnitude": 7.471999168395996,
        "relative": 0.0598166061308714,
        "shape": [
          2048,
          11008
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.27.mlp.gate_proj.weight": {
        "magnitude": 9.857069969177246,
        "relative": 0.07907134191264893,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.27.mlp.up_proj.weight": {
        "magnitude": 7.688497066497803,
        "relative": 0.059201676640607256,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.27.post_attention_layernorm.weight": {
        "magnitude": 0.0701565146446228,
        "relative": 0.0017313019019601407,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.27.self_attn.k_proj.bias": {
        "magnitude": 0.018914153799414635,
        "relative": 0.0002750149222657604,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.27.self_attn.k_proj.weight": {
        "magnitude": 1.3867183923721313,
        "relative": 0.08641416792015112,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.27.self_attn.o_proj.weight": {
        "magnitude": 2.670790910720825,
        "relative": 0.052500358424632895,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.27.self_attn.q_proj.bias": {
        "magnitude": 0.03375016525387764,
        "relative": 0.00014940071413090464,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.27.self_attn.q_proj.weight": {
        "magnitude": 3.8637168407440186,
        "relative": 0.07566233853182264,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.27.self_attn.v_proj.bias": {
        "magnitude": 0.004416467621922493,
        "relative": 0.0024250100210692896,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.27.self_attn.v_proj.weight": {
        "magnitude": 0.9071666598320007,
        "relative": 0.045439923896683755,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.28.input_layernorm.weight": {
        "magnitude": 0.06562846153974533,
        "relative": 0.0014471919081987126,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "layernorm"
      },
      "model.layers.28.mlp.down_proj.weight": {
        "magnitude": 7.474244594573975,
        "relative": 0.05953755353892882,
        "shape": [
          2048,
          11008
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.28.mlp.gate_proj.weight": {
        "magnitude": 9.507057189941406,
        "relative": 0.07522713618991937,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.28.mlp.up_proj.weight": {
        "magnitude": 7.455965042114258,
        "relative": 0.0572894200962145,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.28.post_attention_layernorm.weight": {
        "magnitude": 0.06847786158323288,
        "relative": 0.0016326002869018839,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.28.self_attn.k_proj.bias": {
        "magnitude": 0.026326807215809822,
        "relative": 0.001986499335687791,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.28.self_attn.k_proj.weight": {
        "magnitude": 1.3330632448196411,
        "relative": 0.07744189352603266,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.28.self_attn.o_proj.weight": {
        "magnitude": 3.770989179611206,
        "relative": 0.07108223877088142,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.28.self_attn.q_proj.bias": {
        "magnitude": 0.03194218501448631,
        "relative": 0.0005113342475601683,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.28.self_attn.q_proj.weight": {
        "magnitude": 4.262605667114258,
        "relative": 0.08051248964798183,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.28.self_attn.v_proj.bias": {
        "magnitude": 0.005373723339289427,
        "relative": 0.004546363530338137,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.28.self_attn.v_proj.weight": {
        "magnitude": 0.8857042193412781,
        "relative": 0.040503949749032514,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.29.input_layernorm.weight": {
        "magnitude": 0.04756174609065056,
        "relative": 0.0010003349176935976,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "layernorm"
      },
      "model.layers.29.mlp.down_proj.weight": {
        "magnitude": 6.687714099884033,
        "relative": 0.052977858008442937,
        "shape": [
          2048,
          11008
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.29.mlp.gate_proj.weight": {
        "magnitude": 8.969094276428223,
        "relative": 0.07100724316162091,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.29.mlp.up_proj.weight": {
        "magnitude": 6.702568531036377,
        "relative": 0.051311895050132963,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.29.post_attention_layernorm.weight": {
        "magnitude": 0.06629186123609543,
        "relative": 0.0014949642993855958,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.29.self_attn.k_proj.bias": {
        "magnitude": 0.01955341175198555,
        "relative": 0.0007250496518206305,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.29.self_attn.k_proj.weight": {
        "magnitude": 1.4007658958435059,
        "relative": 0.08232520910382446,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.29.self_attn.o_proj.weight": {
        "magnitude": 3.1060750484466553,
        "relative": 0.05874138831582836,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.29.self_attn.q_proj.bias": {
        "magnitude": 0.029458418488502502,
        "relative": 0.0004974698051255838,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.29.self_attn.q_proj.weight": {
        "magnitude": 4.023470878601074,
        "relative": 0.07765296320743108,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.29.self_attn.v_proj.bias": {
        "magnitude": 0.004654307384043932,
        "relative": 0.003151890407359381,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.29.self_attn.v_proj.weight": {
        "magnitude": 0.794949471950531,
        "relative": 0.036480255505462585,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.3.input_layernorm.weight": {
        "magnitude": 0.013088270090520382,
        "relative": 0.0004237512947001977,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "layernorm"
      },
      "model.layers.3.mlp.down_proj.weight": {
        "magnitude": 1.996423602104187,
        "relative": 0.02451442964295841,
        "shape": [
          2048,
          11008
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.3.mlp.gate_proj.weight": {
        "magnitude": 2.2753634452819824,
        "relative": 0.0289337683406305,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.3.mlp.up_proj.weight": {
        "magnitude": 1.9358006715774536,
        "relative": 0.022720885705655038,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.3.post_attention_layernorm.weight": {
        "magnitude": 0.001975304214283824,
        "relative": 3.2379286518612604e-05,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.3.self_attn.k_proj.bias": {
        "magnitude": 0.013606528751552105,
        "relative": 0.0009230307075150574,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.3.self_attn.k_proj.weight": {
        "magnitude": 0.5833755135536194,
        "relative": 0.022219142293543465,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.3.self_attn.o_proj.weight": {
        "magnitude": 1.5868128538131714,
        "relative": 0.03500366456695012,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.3.self_attn.q_proj.bias": {
        "magnitude": 0.028617974370718002,
        "relative": 0.00046379560533624226,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.3.self_attn.q_proj.weight": {
        "magnitude": 1.9019358158111572,
        "relative": 0.032349484666968534,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.3.self_attn.v_proj.bias": {
        "magnitude": 0.005140822380781174,
        "relative": 0.008525437284461946,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.3.self_attn.v_proj.weight": {
        "magnitude": 0.4697816073894501,
        "relative": 0.039433737849532496,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.30.input_layernorm.weight": {
        "magnitude": 0.050654325634241104,
        "relative": 0.0011479498647142172,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "layernorm"
      },
      "model.layers.30.mlp.down_proj.weight": {
        "magnitude": 6.101706027984619,
        "relative": 0.04803779271497662,
        "shape": [
          2048,
          11008
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.30.mlp.gate_proj.weight": {
        "magnitude": 8.389556884765625,
        "relative": 0.06728290881682639,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.30.mlp.up_proj.weight": {
        "magnitude": 6.1623945236206055,
        "relative": 0.04663886227291511,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.30.post_attention_layernorm.weight": {
        "magnitude": 0.05196988955140114,
        "relative": 0.0011251918679849751,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.30.self_attn.k_proj.bias": {
        "magnitude": 0.011904864571988583,
        "relative": 0.001138235621007014,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.30.self_attn.k_proj.weight": {
        "magnitude": 1.0421767234802246,
        "relative": 0.06822649863404177,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.30.self_attn.o_proj.weight": {
        "magnitude": 2.2587673664093018,
        "relative": 0.041500225498107265,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.30.self_attn.q_proj.bias": {
        "magnitude": 0.026924820616841316,
        "relative": 0.0004193162488611107,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.30.self_attn.q_proj.weight": {
        "magnitude": 3.4360382556915283,
        "relative": 0.06488915533289305,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.30.self_attn.v_proj.bias": {
        "magnitude": 0.003366295248270035,
        "relative": 0.0021377114389617394,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.30.self_attn.v_proj.weight": {
        "magnitude": 0.7147957682609558,
        "relative": 0.03275955366047756,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.31.input_layernorm.weight": {
        "magnitude": 0.041524723172187805,
        "relative": 0.0009735938647019633,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "layernorm"
      },
      "model.layers.31.mlp.down_proj.weight": {
        "magnitude": 5.124398708343506,
        "relative": 0.039426563966110696,
        "shape": [
          2048,
          11008
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.31.mlp.gate_proj.weight": {
        "magnitude": 7.247203350067139,
        "relative": 0.05877839270776861,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.31.mlp.up_proj.weight": {
        "magnitude": 5.342600345611572,
        "relative": 0.039609393035487946,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.31.post_attention_layernorm.weight": {
        "magnitude": 0.02066996321082115,
        "relative": 0.00041428423914056207,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.31.self_attn.k_proj.bias": {
        "magnitude": 0.02687045745551586,
        "relative": 0.0014775506007634381,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.31.self_attn.k_proj.weight": {
        "magnitude": 0.815800130367279,
        "relative": 0.05193620382864132,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.31.self_attn.o_proj.weight": {
        "magnitude": 1.6917966604232788,
        "relative": 0.03112698847973726,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.31.self_attn.q_proj.bias": {
        "magnitude": 0.02346406690776348,
        "relative": 0.0004334743011063717,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.31.self_attn.q_proj.weight": {
        "magnitude": 2.9843320846557617,
        "relative": 0.055420986388299405,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.31.self_attn.v_proj.bias": {
        "magnitude": 0.003072296967729926,
        "relative": 0.0004167145522927519,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.31.self_attn.v_proj.weight": {
        "magnitude": 0.6251733303070068,
        "relative": 0.030465221931655035,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.32.input_layernorm.weight": {
        "magnitude": 0.03032562881708145,
        "relative": 0.0006232945264544686,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "layernorm"
      },
      "model.layers.32.mlp.down_proj.weight": {
        "magnitude": 4.8852338790893555,
        "relative": 0.037160109994614075,
        "shape": [
          2048,
          11008
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.32.mlp.gate_proj.weight": {
        "magnitude": 6.468621253967285,
        "relative": 0.05217644491488194,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.32.mlp.up_proj.weight": {
        "magnitude": 5.031544208526611,
        "relative": 0.03734679405856868,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.32.post_attention_layernorm.weight": {
        "magnitude": 0.019630907103419304,
        "relative": 0.00037210490521195287,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.32.self_attn.k_proj.bias": {
        "magnitude": 0.015533754602074623,
        "relative": 0.0008878311357830085,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.32.self_attn.k_proj.weight": {
        "magnitude": 0.8792877197265625,
        "relative": 0.06183731122192866,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.32.self_attn.o_proj.weight": {
        "magnitude": 1.5523666143417358,
        "relative": 0.025434095417161108,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.32.self_attn.q_proj.bias": {
        "magnitude": 0.02507796883583069,
        "relative": 0.0004896987016071604,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.32.self_attn.q_proj.weight": {
        "magnitude": 2.9342403411865234,
        "relative": 0.055018085943509376,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.32.self_attn.v_proj.bias": {
        "magnitude": 0.0032558301463723183,
        "relative": 0.00043351605851409445,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.32.self_attn.v_proj.weight": {
        "magnitude": 0.42481938004493713,
        "relative": 0.019044388429254197,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.33.input_layernorm.weight": {
        "magnitude": 0.030226286500692368,
        "relative": 0.000549476198394691,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "layernorm"
      },
      "model.layers.33.mlp.down_proj.weight": {
        "magnitude": 4.51909065246582,
        "relative": 0.03404955816033214,
        "shape": [
          2048,
          11008
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.33.mlp.gate_proj.weight": {
        "magnitude": 5.862508773803711,
        "relative": 0.04783547503263438,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.33.mlp.up_proj.weight": {
        "magnitude": 4.76252555847168,
        "relative": 0.03495656313211465,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.33.post_attention_layernorm.weight": {
        "magnitude": 0.001961431233212352,
        "relative": 3.694530863457875e-05,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.33.self_attn.k_proj.bias": {
        "magnitude": 0.02836645022034645,
        "relative": 0.0020942527691105994,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.33.self_attn.k_proj.weight": {
        "magnitude": 0.8655734062194824,
        "relative": 0.06569504123195516,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.33.self_attn.o_proj.weight": {
        "magnitude": 1.4291167259216309,
        "relative": 0.021703209309063946,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.33.self_attn.q_proj.bias": {
        "magnitude": 0.023524034768342972,
        "relative": 0.0004456083308628179,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.33.self_attn.q_proj.weight": {
        "magnitude": 2.759554862976074,
        "relative": 0.057022870833755485,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.33.self_attn.v_proj.bias": {
        "magnitude": 0.0024930264335125685,
        "relative": 0.00041594445029044384,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.33.self_attn.v_proj.weight": {
        "magnitude": 0.3312315046787262,
        "relative": 0.009407019098331722,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.34.input_layernorm.weight": {
        "magnitude": 0.007873298600316048,
        "relative": 0.00013751275315337867,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "layernorm"
      },
      "model.layers.34.mlp.down_proj.weight": {
        "magnitude": 3.884826898574829,
        "relative": 0.029974685429279387,
        "shape": [
          2048,
          11008
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.34.mlp.gate_proj.weight": {
        "magnitude": 5.320844650268555,
        "relative": 0.043322995195132,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.34.mlp.up_proj.weight": {
        "magnitude": 4.343425273895264,
        "relative": 0.032335623353339206,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.34.post_attention_layernorm.weight": {
        "magnitude": 0.00036162094329483807,
        "relative": 5.939863626817386e-06,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.34.self_attn.k_proj.bias": {
        "magnitude": 0.017795203253626823,
        "relative": 0.0009532644263280451,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.34.self_attn.k_proj.weight": {
        "magnitude": 0.817827045917511,
        "relative": 0.05738948962886271,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.34.self_attn.o_proj.weight": {
        "magnitude": 1.363999843597412,
        "relative": 0.024571098494951933,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.34.self_attn.q_proj.bias": {
        "magnitude": 0.02257971651852131,
        "relative": 0.00039782481811401644,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.34.self_attn.q_proj.weight": {
        "magnitude": 2.617086410522461,
        "relative": 0.05640222633732075,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.34.self_attn.v_proj.bias": {
        "magnitude": 0.0023262680042535067,
        "relative": 0.00024177244536861597,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.34.self_attn.v_proj.weight": {
        "magnitude": 0.503308892250061,
        "relative": 0.018807401830800263,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.35.input_layernorm.weight": {
        "magnitude": 0.0044751716777682304,
        "relative": 7.368468086602406e-05,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "layernorm"
      },
      "model.layers.35.mlp.down_proj.weight": {
        "magnitude": 3.2898237705230713,
        "relative": 0.028173545769919827,
        "shape": [
          2048,
          11008
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.35.mlp.gate_proj.weight": {
        "magnitude": 4.920422554016113,
        "relative": 0.039259970758694446,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.35.mlp.up_proj.weight": {
        "magnitude": 3.9746482372283936,
        "relative": 0.030685535689891803,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.35.post_attention_layernorm.weight": {
        "magnitude": 0.0015869140625,
        "relative": 2.0787778862082288e-05,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.35.self_attn.k_proj.bias": {
        "magnitude": 0.00980730727314949,
        "relative": 0.0004037745367362496,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.35.self_attn.k_proj.weight": {
        "magnitude": 0.7186992168426514,
        "relative": 0.05086973102922877,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.35.self_attn.o_proj.weight": {
        "magnitude": 1.2001192569732666,
        "relative": 0.02143047181945158,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.35.self_attn.q_proj.bias": {
        "magnitude": 0.019341450184583664,
        "relative": 0.0003936036762634837,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.35.self_attn.q_proj.weight": {
        "magnitude": 2.3555731773376465,
        "relative": 0.05111887745199042,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.35.self_attn.v_proj.bias": {
        "magnitude": 0.0016065990785136819,
        "relative": 0.00012123760326769976,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.35.self_attn.v_proj.weight": {
        "magnitude": 0.4413440525531769,
        "relative": 0.01741217664623507,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.4.input_layernorm.weight": {
        "magnitude": 0.027726972475647926,
        "relative": 0.0012093954618733218,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "layernorm"
      },
      "model.layers.4.mlp.down_proj.weight": {
        "magnitude": 2.394425868988037,
        "relative": 0.028625606282127524,
        "shape": [
          2048,
          11008
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.4.mlp.gate_proj.weight": {
        "magnitude": 2.6606078147888184,
        "relative": 0.029927539048708048,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.4.mlp.up_proj.weight": {
        "magnitude": 2.2049386501312256,
        "relative": 0.02595626919656955,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.4.post_attention_layernorm.weight": {
        "magnitude": 0.007324911188334227,
        "relative": 0.00014640012252005116,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.4.self_attn.k_proj.bias": {
        "magnitude": 0.017551597207784653,
        "relative": 0.0016785955370406584,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.4.self_attn.k_proj.weight": {
        "magnitude": 0.5335236191749573,
        "relative": 0.021333161617988247,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.4.self_attn.o_proj.weight": {
        "magnitude": 1.7964942455291748,
        "relative": 0.03754011861645984,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.4.self_attn.q_proj.bias": {
        "magnitude": 0.0344524122774601,
        "relative": 0.0005285134224122535,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.4.self_attn.q_proj.weight": {
        "magnitude": 2.0712532997131348,
        "relative": 0.03499213723511291,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.4.self_attn.v_proj.bias": {
        "magnitude": 0.005322678945958614,
        "relative": 0.008402397669951853,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.4.self_attn.v_proj.weight": {
        "magnitude": 0.4477541148662567,
        "relative": 0.028401803353257093,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.5.input_layernorm.weight": {
        "magnitude": 0.030483346432447433,
        "relative": 0.0010960959804799475,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "layernorm"
      },
      "model.layers.5.mlp.down_proj.weight": {
        "magnitude": 3.374962568283081,
        "relative": 0.030964992339263902,
        "shape": [
          2048,
          11008
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.5.mlp.gate_proj.weight": {
        "magnitude": 3.556318998336792,
        "relative": 0.03113583950577896,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.5.mlp.up_proj.weight": {
        "magnitude": 2.957623243331909,
        "relative": 0.02676870998252188,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.5.post_attention_layernorm.weight": {
        "magnitude": 0.009570080786943436,
        "relative": 0.00020271253008227245,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.5.self_attn.k_proj.bias": {
        "magnitude": 0.015637602657079697,
        "relative": 0.002168582472405657,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.5.self_attn.k_proj.weight": {
        "magnitude": 0.6002610325813293,
        "relative": 0.024982855564628783,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.5.self_attn.o_proj.weight": {
        "magnitude": 1.89191734790802,
        "relative": 0.04130228149635891,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.5.self_attn.q_proj.bias": {
        "magnitude": 0.03439369425177574,
        "relative": 0.00045840636949641736,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.5.self_attn.q_proj.weight": {
        "magnitude": 2.1897757053375244,
        "relative": 0.037751629984001715,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.5.self_attn.v_proj.bias": {
        "magnitude": 0.004024411551654339,
        "relative": 0.003612067565913248,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.5.self_attn.v_proj.weight": {
        "magnitude": 0.5296964645385742,
        "relative": 0.030696088659420303,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.6.input_layernorm.weight": {
        "magnitude": 0.024682946503162384,
        "relative": 0.0009722442610272604,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "layernorm"
      },
      "model.layers.6.mlp.down_proj.weight": {
        "magnitude": 3.9446582794189453,
        "relative": 0.03294432191930585,
        "shape": [
          2048,
          11008
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.6.mlp.gate_proj.weight": {
        "magnitude": 4.2792181968688965,
        "relative": 0.03414096233924156,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.6.mlp.up_proj.weight": {
        "magnitude": 3.5007071495056152,
        "relative": 0.02896143401071997,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.6.post_attention_layernorm.weight": {
        "magnitude": 0.009162090718746185,
        "relative": 0.00018374338252079308,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.6.self_attn.k_proj.bias": {
        "magnitude": 0.01747259870171547,
        "relative": 0.0020041451287306014,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.6.self_attn.k_proj.weight": {
        "magnitude": 0.7242220640182495,
        "relative": 0.030775688687506176,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.6.self_attn.o_proj.weight": {
        "magnitude": 1.8586246967315674,
        "relative": 0.03668365191212499,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.6.self_attn.q_proj.bias": {
        "magnitude": 0.029208051040768623,
        "relative": 0.0004567952039334285,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.6.self_attn.q_proj.weight": {
        "magnitude": 2.304426431655884,
        "relative": 0.03942259111106082,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.6.self_attn.v_proj.bias": {
        "magnitude": 0.006031055003404617,
        "relative": 0.01352639918083707,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.6.self_attn.v_proj.weight": {
        "magnitude": 0.5489028096199036,
        "relative": 0.03389029360419488,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.7.input_layernorm.weight": {
        "magnitude": 0.04926030710339546,
        "relative": 0.0015986639941316016,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "layernorm"
      },
      "model.layers.7.mlp.down_proj.weight": {
        "magnitude": 4.168181419372559,
        "relative": 0.03619492223100875,
        "shape": [
          2048,
          11008
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.7.mlp.gate_proj.weight": {
        "magnitude": 4.663689613342285,
        "relative": 0.03739688739053455,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.7.mlp.up_proj.weight": {
        "magnitude": 3.782726764678955,
        "relative": 0.03260445108968081,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.7.post_attention_layernorm.weight": {
        "magnitude": 0.009575488977134228,
        "relative": 0.00018196459320552383,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.7.self_attn.k_proj.bias": {
        "magnitude": 0.021256960928440094,
        "relative": 0.0025247752559234686,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.7.self_attn.k_proj.weight": {
        "magnitude": 1.0167913436889648,
        "relative": 0.05198564737619402,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.7.self_attn.o_proj.weight": {
        "magnitude": 2.5294113159179688,
        "relative": 0.05050669317220832,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.7.self_attn.q_proj.bias": {
        "magnitude": 0.045816902071237564,
        "relative": 0.0008131619739260674,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.7.self_attn.q_proj.weight": {
        "magnitude": 3.014077663421631,
        "relative": 0.054829587573700024,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.7.self_attn.v_proj.bias": {
        "magnitude": 0.004855574574321508,
        "relative": 0.01096203223080939,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.7.self_attn.v_proj.weight": {
        "magnitude": 0.7824987173080444,
        "relative": 0.04380290586068674,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.8.input_layernorm.weight": {
        "magnitude": 0.05699743703007698,
        "relative": 0.001814888074344807,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "layernorm"
      },
      "model.layers.8.mlp.down_proj.weight": {
        "magnitude": 4.792405128479004,
        "relative": 0.0401487478340438,
        "shape": [
          2048,
          11008
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.8.mlp.gate_proj.weight": {
        "magnitude": 5.473804950714111,
        "relative": 0.03866116238327093,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.8.mlp.up_proj.weight": {
        "magnitude": 4.394013404846191,
        "relative": 0.03643446520504929,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.8.post_attention_layernorm.weight": {
        "magnitude": 0.014875867404043674,
        "relative": 0.000286838188421926,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.8.self_attn.k_proj.bias": {
        "magnitude": 0.02060686983168125,
        "relative": 0.002575430746455084,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.8.self_attn.k_proj.weight": {
        "magnitude": 1.072048544883728,
        "relative": 0.049356182709476816,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.8.self_attn.o_proj.weight": {
        "magnitude": 2.7443971633911133,
        "relative": 0.05373669507785083,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.8.self_attn.q_proj.bias": {
        "magnitude": 0.046669792383909225,
        "relative": 0.0007892707724496932,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.8.self_attn.q_proj.weight": {
        "magnitude": 3.1285195350646973,
        "relative": 0.05603146043520896,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.8.self_attn.v_proj.bias": {
        "magnitude": 0.0049222433008253574,
        "relative": 0.01072224871461421,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.8.self_attn.v_proj.weight": {
        "magnitude": 0.8410707116127014,
        "relative": 0.046737229261061275,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.9.input_layernorm.weight": {
        "magnitude": 0.06326323747634888,
        "relative": 0.002299613539309139,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "layernorm"
      },
      "model.layers.9.mlp.down_proj.weight": {
        "magnitude": 4.777820110321045,
        "relative": 0.041067589315617506,
        "shape": [
          2048,
          11008
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.9.mlp.gate_proj.weight": {
        "magnitude": 5.822098255157471,
        "relative": 0.0406341463055099,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.9.mlp.up_proj.weight": {
        "magnitude": 4.518120765686035,
        "relative": 0.0381326146990448,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.9.post_attention_layernorm.weight": {
        "magnitude": 0.018323298543691635,
        "relative": 0.0003727260925232447,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.9.self_attn.k_proj.bias": {
        "magnitude": 0.019716087728738785,
        "relative": 0.002232660831016275,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.9.self_attn.k_proj.weight": {
        "magnitude": 1.0018833875656128,
        "relative": 0.041893251775012165,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.9.self_attn.o_proj.weight": {
        "magnitude": 2.7357091903686523,
        "relative": 0.05334006030409031,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.9.self_attn.q_proj.bias": {
        "magnitude": 0.04175400733947754,
        "relative": 0.0006251419989398107,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.9.self_attn.q_proj.weight": {
        "magnitude": 3.1637728214263916,
        "relative": 0.0548486707189394,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.9.self_attn.v_proj.bias": {
        "magnitude": 0.005148179829120636,
        "relative": 0.0077875613868230165,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.9.self_attn.v_proj.weight": {
        "magnitude": 0.7884073257446289,
        "relative": 0.04223131831295473,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.norm.weight": {
        "magnitude": 0.0008802615338936448,
        "relative": 6.053548279927785e-06,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "other"
      }
    },
    "cpt_to_sft": {
      "lm_head.weight": {
        "magnitude": 4.074172019958496,
        "relative": 0.010009215711607978,
        "shape": [
          151936,
          2048
        ],
        "param_count": 311164928,
        "layer_type": "output"
      },
      "model.embed_tokens.weight": {
        "magnitude": 4.074172019958496,
        "relative": 0.010009215711607978,
        "shape": [
          151936,
          2048
        ],
        "param_count": 311164928,
        "layer_type": "embedding"
      },
      "model.layers.0.input_layernorm.weight": {
        "magnitude": 0.0,
        "relative": 0.0,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "layernorm"
      },
      "model.layers.0.mlp.down_proj.weight": {
        "magnitude": 0.7000254988670349,
        "relative": 0.005833339617603731,
        "shape": [
          2048,
          11008
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.0.mlp.gate_proj.weight": {
        "magnitude": 0.6649388670921326,
        "relative": 0.0052283210270576835,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.0.mlp.up_proj.weight": {
        "magnitude": 0.6535710692405701,
        "relative": 0.005843438522882378,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.0.post_attention_layernorm.weight": {
        "magnitude": 0.0014504680875688791,
        "relative": 7.783707547300703e-05,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.0.self_attn.k_proj.bias": {
        "magnitude": 0.00201416015625,
        "relative": 8.206031569636793e-06,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.0.self_attn.k_proj.weight": {
        "magnitude": 0.08751203864812851,
        "relative": 0.002367465598387538,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.0.self_attn.o_proj.weight": {
        "magnitude": 0.25411057472229004,
        "relative": 0.005751636075078143,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.0.self_attn.q_proj.bias": {
        "magnitude": 0.004408320877701044,
        "relative": 7.416712868438652e-05,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.0.self_attn.q_proj.weight": {
        "magnitude": 0.26646631956100464,
        "relative": 0.0035554463923220844,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.0.self_attn.v_proj.bias": {
        "magnitude": 0.0024125599302351475,
        "relative": 0.0009158607881365066,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.0.self_attn.v_proj.weight": {
        "magnitude": 0.07551293820142746,
        "relative": 0.006311612097322674,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.1.input_layernorm.weight": {
        "magnitude": 0.0025312674697488546,
        "relative": 0.0002024237773385547,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "layernorm"
      },
      "model.layers.1.mlp.down_proj.weight": {
        "magnitude": 0.27305886149406433,
        "relative": 0.0062271252233517356,
        "shape": [
          2048,
          11008
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.1.mlp.gate_proj.weight": {
        "magnitude": 0.2767898440361023,
        "relative": 0.004861470702503944,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.1.mlp.up_proj.weight": {
        "magnitude": 0.2016453742980957,
        "relative": 0.004484332654937067,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.1.post_attention_layernorm.weight": {
        "magnitude": 0.00030750935547985137,
        "relative": 5.22678870171709e-06,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.1.self_attn.k_proj.bias": {
        "magnitude": 0.0010067332768812776,
        "relative": 1.3938326802327698e-05,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.1.self_attn.k_proj.weight": {
        "magnitude": 0.13334079086780548,
        "relative": 0.004346726529366956,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.1.self_attn.o_proj.weight": {
        "magnitude": 0.3220045566558838,
        "relative": 0.007050952398182937,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.1.self_attn.q_proj.bias": {
        "magnitude": 0.00508594186976552,
        "relative": 0.00014992650083914917,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.1.self_attn.q_proj.weight": {
        "magnitude": 0.3628332316875458,
        "relative": 0.00613270488023638,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.1.self_attn.v_proj.bias": {
        "magnitude": 0.0016752482624724507,
        "relative": 0.0006923433383492232,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.1.self_attn.v_proj.weight": {
        "magnitude": 0.10586202889680862,
        "relative": 0.00792138293103847,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.10.input_layernorm.weight": {
        "magnitude": 0.00011603516759350896,
        "relative": 3.95576252506502e-06,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "layernorm"
      },
      "model.layers.10.mlp.down_proj.weight": {
        "magnitude": 0.9087377786636353,
        "relative": 0.007871548656936156,
        "shape": [
          2048,
          11008
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.10.mlp.gate_proj.weight": {
        "magnitude": 0.9845859408378601,
        "relative": 0.006773926664092972,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.10.mlp.up_proj.weight": {
        "magnitude": 0.887978732585907,
        "relative": 0.007500680597343694,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.10.post_attention_layernorm.weight": {
        "magnitude": 4.9736074288375676e-05,
        "relative": 1.1560800905374666e-06,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.10.self_attn.k_proj.bias": {
        "magnitude": 0.0023909115698188543,
        "relative": 0.00023916521617731794,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.10.self_attn.k_proj.weight": {
        "magnitude": 0.13491731882095337,
        "relative": 0.00676626521652367,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.10.self_attn.o_proj.weight": {
        "magnitude": 0.39185094833374023,
        "relative": 0.007635689635075377,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.10.self_attn.q_proj.bias": {
        "magnitude": 0.0036969375796616077,
        "relative": 5.798843299299064e-05,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.10.self_attn.q_proj.weight": {
        "magnitude": 0.3961285352706909,
        "relative": 0.007303033107870904,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.10.self_attn.v_proj.bias": {
        "magnitude": 0.0012969550443813205,
        "relative": 0.001310933163553365,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.10.self_attn.v_proj.weight": {
        "magnitude": 0.134633406996727,
        "relative": 0.006883275229030261,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.11.input_layernorm.weight": {
        "magnitude": 0.001476967940106988,
        "relative": 5.046606283769728e-05,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "layernorm"
      },
      "model.layers.11.mlp.down_proj.weight": {
        "magnitude": 0.9451262950897217,
        "relative": 0.00795291061173773,
        "shape": [
          2048,
          11008
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.11.mlp.gate_proj.weight": {
        "magnitude": 1.0027658939361572,
        "relative": 0.007114966337331767,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.11.mlp.up_proj.weight": {
        "magnitude": 0.9304885268211365,
        "relative": 0.007664397646620106,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.11.post_attention_layernorm.weight": {
        "magnitude": 0.00014017036301083863,
        "relative": 4.091595127563569e-06,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.11.self_attn.k_proj.bias": {
        "magnitude": 0.002533258404582739,
        "relative": 0.00022706874304547492,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.11.self_attn.k_proj.weight": {
        "magnitude": 0.14048004150390625,
        "relative": 0.006892372878370853,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.11.self_attn.o_proj.weight": {
        "magnitude": 0.4235154092311859,
        "relative": 0.007971184743781126,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.11.self_attn.q_proj.bias": {
        "magnitude": 0.004257941618561745,
        "relative": 6.377682032282054e-05,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.11.self_attn.q_proj.weight": {
        "magnitude": 0.4141242802143097,
        "relative": 0.007473396572976694,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.11.self_attn.v_proj.bias": {
        "magnitude": 0.0008694795542396605,
        "relative": 0.0007901335801375844,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.11.self_attn.v_proj.weight": {
        "magnitude": 0.14432844519615173,
        "relative": 0.007319145314791025,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.12.input_layernorm.weight": {
        "magnitude": 0.0013841241598129272,
        "relative": 4.381840324640027e-05,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "layernorm"
      },
      "model.layers.12.mlp.down_proj.weight": {
        "magnitude": 0.9639630317687988,
        "relative": 0.008175617132773861,
        "shape": [
          2048,
          11008
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.12.mlp.gate_proj.weight": {
        "magnitude": 1.0494332313537598,
        "relative": 0.007314379645512698,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.12.mlp.up_proj.weight": {
        "magnitude": 0.9506666660308838,
        "relative": 0.007869198345226409,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.12.post_attention_layernorm.weight": {
        "magnitude": 1.1920928955078125e-06,
        "relative": 3.135471848056193e-08,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.12.self_attn.k_proj.bias": {
        "magnitude": 0.001902310410514474,
        "relative": 0.0001649724096638224,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.12.self_attn.k_proj.weight": {
        "magnitude": 0.14112259447574615,
        "relative": 0.006597821676343595,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.12.self_attn.o_proj.weight": {
        "magnitude": 0.4207235276699066,
        "relative": 0.007940681941354373,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.12.self_attn.q_proj.bias": {
        "magnitude": 0.0033142322208732367,
        "relative": 5.597728884084029e-05,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.12.self_attn.q_proj.weight": {
        "magnitude": 0.42034366726875305,
        "relative": 0.007588071184995125,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.12.self_attn.v_proj.bias": {
        "magnitude": 0.00098247523419559,
        "relative": 0.000576507940975786,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.12.self_attn.v_proj.weight": {
        "magnitude": 0.15013930201530457,
        "relative": 0.007887139803500294,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.13.input_layernorm.weight": {
        "magnitude": 0.00018310546875,
        "relative": 6.647516277769506e-06,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "layernorm"
      },
      "model.layers.13.mlp.down_proj.weight": {
        "magnitude": 1.0016224384307861,
        "relative": 0.007966313074317907,
        "shape": [
          2048,
          11008
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.13.mlp.gate_proj.weight": {
        "magnitude": 1.0369186401367188,
        "relative": 0.007948027498392825,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.13.mlp.up_proj.weight": {
        "magnitude": 0.9841740131378174,
        "relative": 0.007685454960981399,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.13.post_attention_layernorm.weight": {
        "magnitude": 0.0001366055221296847,
        "relative": 4.025279789600576e-06,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.13.self_attn.k_proj.bias": {
        "magnitude": 0.0025340195279568434,
        "relative": 0.0002445123291759994,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.13.self_attn.k_proj.weight": {
        "magnitude": 0.14355213940143585,
        "relative": 0.005727876008314748,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.13.self_attn.o_proj.weight": {
        "magnitude": 0.4235105812549591,
        "relative": 0.008889335319677368,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.13.self_attn.q_proj.bias": {
        "magnitude": 0.005160943605005741,
        "relative": 7.275493151387186e-05,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.13.self_attn.q_proj.weight": {
        "magnitude": 0.43439042568206787,
        "relative": 0.007448965604815324,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.13.self_attn.v_proj.bias": {
        "magnitude": 0.0011490157339721918,
        "relative": 0.0008757147192548113,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.13.self_attn.v_proj.weight": {
        "magnitude": 0.14591580629348755,
        "relative": 0.009195958615345123,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.14.input_layernorm.weight": {
        "magnitude": 0.0007305120234377682,
        "relative": 2.4195016265505335e-05,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "layernorm"
      },
      "model.layers.14.mlp.down_proj.weight": {
        "magnitude": 1.0360158681869507,
        "relative": 0.008303743090589805,
        "shape": [
          2048,
          11008
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.14.mlp.gate_proj.weight": {
        "magnitude": 1.0801557302474976,
        "relative": 0.008284912677492753,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.14.mlp.up_proj.weight": {
        "magnitude": 1.0195151567459106,
        "relative": 0.00797638483265248,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.14.post_attention_layernorm.weight": {
        "magnitude": 0.0014046053402125835,
        "relative": 4.0184405333608915e-05,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.14.self_attn.k_proj.bias": {
        "magnitude": 0.002477796282619238,
        "relative": 0.00021776156979298603,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.14.self_attn.k_proj.weight": {
        "magnitude": 0.1483747959136963,
        "relative": 0.0066526328245445025,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.14.self_attn.o_proj.weight": {
        "magnitude": 0.4494856894016266,
        "relative": 0.008807605363843302,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.14.self_attn.q_proj.bias": {
        "magnitude": 0.004306953866034746,
        "relative": 6.52185632306977e-05,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.14.self_attn.q_proj.weight": {
        "magnitude": 0.45017674565315247,
        "relative": 0.007767573269255875,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.14.self_attn.v_proj.bias": {
        "magnitude": 0.0014357412001118064,
        "relative": 0.0014478497378939401,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.14.self_attn.v_proj.weight": {
        "magnitude": 0.15158669650554657,
        "relative": 0.008171082479846568,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.15.input_layernorm.weight": {
        "magnitude": 0.0012976252473890781,
        "relative": 3.9478215179786435e-05,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "layernorm"
      },
      "model.layers.15.mlp.down_proj.weight": {
        "magnitude": 1.065165400505066,
        "relative": 0.008437003632516234,
        "shape": [
          2048,
          11008
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.15.mlp.gate_proj.weight": {
        "magnitude": 1.0966297388076782,
        "relative": 0.008687808409433355,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.15.mlp.up_proj.weight": {
        "magnitude": 1.050196647644043,
        "relative": 0.008102940457337948,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.15.post_attention_layernorm.weight": {
        "magnitude": 0.0013814857229590416,
        "relative": 3.908877456937438e-05,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.15.self_attn.k_proj.bias": {
        "magnitude": 0.0022042489144951105,
        "relative": 0.0001951950798211023,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.15.self_attn.k_proj.weight": {
        "magnitude": 0.15064731240272522,
        "relative": 0.0068439002276417865,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.15.self_attn.o_proj.weight": {
        "magnitude": 0.433294415473938,
        "relative": 0.008704935394191484,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.15.self_attn.q_proj.bias": {
        "magnitude": 0.004732745233923197,
        "relative": 7.014319832669155e-05,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.15.self_attn.q_proj.weight": {
        "magnitude": 0.44531527161598206,
        "relative": 0.007766706357916021,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.15.self_attn.v_proj.bias": {
        "magnitude": 0.0011340901255607605,
        "relative": 0.0009289737750127633,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.15.self_attn.v_proj.weight": {
        "magnitude": 0.15240661799907684,
        "relative": 0.008602755950810018,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.16.input_layernorm.weight": {
        "magnitude": 0.001429880503565073,
        "relative": 3.781346132103851e-05,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "layernorm"
      },
      "model.layers.16.mlp.down_proj.weight": {
        "magnitude": 1.106433391571045,
        "relative": 0.009054147630704654,
        "shape": [
          2048,
          11008
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.16.mlp.gate_proj.weight": {
        "magnitude": 1.140446424484253,
        "relative": 0.008879309102346385,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.16.mlp.up_proj.weight": {
        "magnitude": 1.0945839881896973,
        "relative": 0.00868814511473816,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.16.post_attention_layernorm.weight": {
        "magnitude": 8.821487426757812e-06,
        "relative": 2.4491098789392863e-07,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.16.self_attn.k_proj.bias": {
        "magnitude": 0.0017756472807377577,
        "relative": 0.00010621382274225694,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.16.self_attn.k_proj.weight": {
        "magnitude": 0.15846438705921173,
        "relative": 0.007074767571360628,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.16.self_attn.o_proj.weight": {
        "magnitude": 0.47060978412628174,
        "relative": 0.009196302324137973,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.16.self_attn.q_proj.bias": {
        "magnitude": 0.004554223734885454,
        "relative": 7.894397205556402e-05,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.16.self_attn.q_proj.weight": {
        "magnitude": 0.4810311496257782,
        "relative": 0.008283805760876551,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.16.self_attn.v_proj.bias": {
        "magnitude": 0.0013583643594756722,
        "relative": 0.0020062364339058017,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.16.self_attn.v_proj.weight": {
        "magnitude": 0.15646763145923615,
        "relative": 0.008550677090299915,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.17.input_layernorm.weight": {
        "magnitude": 0.00038969822344370186,
        "relative": 9.418624785927584e-06,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "layernorm"
      },
      "model.layers.17.mlp.down_proj.weight": {
        "magnitude": 1.1142723560333252,
        "relative": 0.009135959506125245,
        "shape": [
          2048,
          11008
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.17.mlp.gate_proj.weight": {
        "magnitude": 1.1540378332138062,
        "relative": 0.009253642975406116,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.17.mlp.up_proj.weight": {
        "magnitude": 1.107945203781128,
        "relative": 0.008794265991150855,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.17.post_attention_layernorm.weight": {
        "magnitude": 0.0009870534995570779,
        "relative": 2.705288996490216e-05,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.17.self_attn.k_proj.bias": {
        "magnitude": 0.002207547426223755,
        "relative": 0.0002651056648665454,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.17.self_attn.k_proj.weight": {
        "magnitude": 0.15493884682655334,
        "relative": 0.00764287196943864,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.17.self_attn.o_proj.weight": {
        "magnitude": 0.4472443461418152,
        "relative": 0.0092170287811453,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.17.self_attn.q_proj.bias": {
        "magnitude": 0.00555389141663909,
        "relative": 9.387144800203015e-05,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.17.self_attn.q_proj.weight": {
        "magnitude": 0.47360169887542725,
        "relative": 0.00869863876748089,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.17.self_attn.v_proj.bias": {
        "magnitude": 0.0015460157301276922,
        "relative": 0.001132972798423801,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.17.self_attn.v_proj.weight": {
        "magnitude": 0.15973247587680817,
        "relative": 0.009430459671208742,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.18.input_layernorm.weight": {
        "magnitude": 0.0017605230677872896,
        "relative": 4.091729230492065e-05,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "layernorm"
      },
      "model.layers.18.mlp.down_proj.weight": {
        "magnitude": 1.1316720247268677,
        "relative": 0.009408479095918146,
        "shape": [
          2048,
          11008
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.18.mlp.gate_proj.weight": {
        "magnitude": 1.1765915155410767,
        "relative": 0.009354076741367285,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.18.mlp.up_proj.weight": {
        "magnitude": 1.1284393072128296,
        "relative": 0.009081265416065673,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.18.post_attention_layernorm.weight": {
        "magnitude": 0.001760595478117466,
        "relative": 4.72819622399252e-05,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.18.self_attn.k_proj.bias": {
        "magnitude": 0.0022116429172456264,
        "relative": 5.863518934525203e-05,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.18.self_attn.k_proj.weight": {
        "magnitude": 0.16152863204479218,
        "relative": 0.0077996033125482984,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.18.self_attn.o_proj.weight": {
        "magnitude": 0.4645007848739624,
        "relative": 0.009451787562549966,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.18.self_attn.q_proj.bias": {
        "magnitude": 0.005591187626123428,
        "relative": 8.774964071663502e-05,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.18.self_attn.q_proj.weight": {
        "magnitude": 0.4914492666721344,
        "relative": 0.00887312039479073,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.18.self_attn.v_proj.bias": {
        "magnitude": 0.0014271659310907125,
        "relative": 0.002007531365075215,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.18.self_attn.v_proj.weight": {
        "magnitude": 0.1598331183195114,
        "relative": 0.008800717710783255,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.19.input_layernorm.weight": {
        "magnitude": 0.00180337973870337,
        "relative": 3.818004301911635e-05,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "layernorm"
      },
      "model.layers.19.mlp.down_proj.weight": {
        "magnitude": 1.1552753448486328,
        "relative": 0.009502661165432387,
        "shape": [
          2048,
          11008
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.19.mlp.gate_proj.weight": {
        "magnitude": 1.1729494333267212,
        "relative": 0.009748330834793673,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.19.mlp.up_proj.weight": {
        "magnitude": 1.1327840089797974,
        "relative": 0.009221583022725183,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.19.post_attention_layernorm.weight": {
        "magnitude": 0.00013016164302825928,
        "relative": 3.3588229908981028e-06,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.19.self_attn.k_proj.bias": {
        "magnitude": 0.0021034146193414927,
        "relative": 7.96770826444338e-05,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.19.self_attn.k_proj.weight": {
        "magnitude": 0.16326439380645752,
        "relative": 0.007300238131144302,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.19.self_attn.o_proj.weight": {
        "magnitude": 0.471189022064209,
        "relative": 0.00995805895986738,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.19.self_attn.q_proj.bias": {
        "magnitude": 0.00553574925288558,
        "relative": 8.812208348863363e-05,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.19.self_attn.q_proj.weight": {
        "magnitude": 0.4978949725627899,
        "relative": 0.008978610180469892,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.19.self_attn.v_proj.bias": {
        "magnitude": 0.001705419272184372,
        "relative": 0.002042429690027499,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.19.self_attn.v_proj.weight": {
        "magnitude": 0.1624223291873932,
        "relative": 0.010092581785213639,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.2.input_layernorm.weight": {
        "magnitude": 0.0016868864186108112,
        "relative": 9.485013151001853e-05,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "layernorm"
      },
      "model.layers.2.mlp.down_proj.weight": {
        "magnitude": 0.36731261014938354,
        "relative": 0.006260941462462578,
        "shape": [
          2048,
          11008
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.2.mlp.gate_proj.weight": {
        "magnitude": 0.36075013875961304,
        "relative": 0.0063538656913800454,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.2.mlp.up_proj.weight": {
        "magnitude": 0.33621910214424133,
        "relative": 0.005064948933732495,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.2.post_attention_layernorm.weight": {
        "magnitude": 0.00025737006217241287,
        "relative": 4.265732639576137e-06,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.2.self_attn.k_proj.bias": {
        "magnitude": 0.005393954925239086,
        "relative": 0.00011489970802982112,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.2.self_attn.k_proj.weight": {
        "magnitude": 0.12287341058254242,
        "relative": 0.004536004964859793,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.2.self_attn.o_proj.weight": {
        "magnitude": 0.3109767735004425,
        "relative": 0.0068032134139218315,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.2.self_attn.q_proj.bias": {
        "magnitude": 0.005664669908583164,
        "relative": 0.0001684514955357471,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.2.self_attn.q_proj.weight": {
        "magnitude": 0.35211437940597534,
        "relative": 0.006090916874082773,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.2.self_attn.v_proj.bias": {
        "magnitude": 0.0016448560636490583,
        "relative": 0.0044723418677848365,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.2.self_attn.v_proj.weight": {
        "magnitude": 0.10392984002828598,
        "relative": 0.007592331409273921,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.20.input_layernorm.weight": {
        "magnitude": 0.0019742301665246487,
        "relative": 3.42598891931935e-05,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "layernorm"
      },
      "model.layers.20.mlp.down_proj.weight": {
        "magnitude": 1.1657116413116455,
        "relative": 0.009451404774009324,
        "shape": [
          2048,
          11008
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.20.mlp.gate_proj.weight": {
        "magnitude": 1.2074999809265137,
        "relative": 0.009680926933380771,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.20.mlp.up_proj.weight": {
        "magnitude": 1.1644365787506104,
        "relative": 0.009136148320258497,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.20.post_attention_layernorm.weight": {
        "magnitude": 0.0013823535991832614,
        "relative": 3.63432016966338e-05,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.20.self_attn.k_proj.bias": {
        "magnitude": 0.002154584741219878,
        "relative": 7.060308312422578e-05,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.20.self_attn.k_proj.weight": {
        "magnitude": 0.16416339576244354,
        "relative": 0.008943987846600356,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.20.self_attn.o_proj.weight": {
        "magnitude": 0.4700544774532318,
        "relative": 0.009015310046801527,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.20.self_attn.q_proj.bias": {
        "magnitude": 0.005702584981918335,
        "relative": 8.941825122778077e-05,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.20.self_attn.q_proj.weight": {
        "magnitude": 0.5010770559310913,
        "relative": 0.009419515552823901,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.20.self_attn.v_proj.bias": {
        "magnitude": 0.0012240452924743295,
        "relative": 0.0009439333378038214,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.20.self_attn.v_proj.weight": {
        "magnitude": 0.16327014565467834,
        "relative": 0.0092330522135923,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.21.input_layernorm.weight": {
        "magnitude": 0.0025033759884536266,
        "relative": 4.36881970402751e-05,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "layernorm"
      },
      "model.layers.21.mlp.down_proj.weight": {
        "magnitude": 1.1801750659942627,
        "relative": 0.009738938042682737,
        "shape": [
          2048,
          11008
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.21.mlp.gate_proj.weight": {
        "magnitude": 1.219687819480896,
        "relative": 0.009547981928690382,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.21.mlp.up_proj.weight": {
        "magnitude": 1.17774498462677,
        "relative": 0.009388590953630517,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.21.post_attention_layernorm.weight": {
        "magnitude": 0.000999654526822269,
        "relative": 2.563337835985314e-05,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.21.self_attn.k_proj.bias": {
        "magnitude": 0.0017449144506826997,
        "relative": 7.899176769307582e-05,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.21.self_attn.k_proj.weight": {
        "magnitude": 0.17574846744537354,
        "relative": 0.008748301230699895,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.21.self_attn.o_proj.weight": {
        "magnitude": 0.49787038564682007,
        "relative": 0.00982948758506188,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.21.self_attn.q_proj.bias": {
        "magnitude": 0.0046148053370416164,
        "relative": 9.51915203061299e-05,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.21.self_attn.q_proj.weight": {
        "magnitude": 0.5227127075195312,
        "relative": 0.00966760610263363,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.21.self_attn.v_proj.bias": {
        "magnitude": 0.001411254284903407,
        "relative": 0.0019348116993668273,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.21.self_attn.v_proj.weight": {
        "magnitude": 0.17071759700775146,
        "relative": 0.009427704854583361,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.22.input_layernorm.weight": {
        "magnitude": 0.00034536782186478376,
        "relative": 6.757081585100807e-06,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "layernorm"
      },
      "model.layers.22.mlp.down_proj.weight": {
        "magnitude": 1.1906957626342773,
        "relative": 0.009800919044505476,
        "shape": [
          2048,
          11008
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.22.mlp.gate_proj.weight": {
        "magnitude": 1.2281211614608765,
        "relative": 0.009721567274149223,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.22.mlp.up_proj.weight": {
        "magnitude": 1.1863335371017456,
        "relative": 0.009430419290395295,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.22.post_attention_layernorm.weight": {
        "magnitude": 0.0019751330837607384,
        "relative": 4.985052495184054e-05,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.22.self_attn.k_proj.bias": {
        "magnitude": 0.002267676405608654,
        "relative": 0.00016701265979437616,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.22.self_attn.k_proj.weight": {
        "magnitude": 0.17384028434753418,
        "relative": 0.00921229873023395,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.22.self_attn.o_proj.weight": {
        "magnitude": 0.490797758102417,
        "relative": 0.00950513256796732,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.22.self_attn.q_proj.bias": {
        "magnitude": 0.005129438824951649,
        "relative": 9.366123289021353e-05,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.22.self_attn.q_proj.weight": {
        "magnitude": 0.517650306224823,
        "relative": 0.009520861928306402,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.22.self_attn.v_proj.bias": {
        "magnitude": 0.0014730055117979646,
        "relative": 0.0016131809100615139,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.22.self_attn.v_proj.weight": {
        "magnitude": 0.16953854262828827,
        "relative": 0.008923532671796052,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.23.input_layernorm.weight": {
        "magnitude": 0.0019569359719753265,
        "relative": 4.118572261929283e-05,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "layernorm"
      },
      "model.layers.23.mlp.down_proj.weight": {
        "magnitude": 1.181889533996582,
        "relative": 0.009702706432826226,
        "shape": [
          2048,
          11008
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.23.mlp.gate_proj.weight": {
        "magnitude": 1.226951241493225,
        "relative": 0.01003945096117919,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.23.mlp.up_proj.weight": {
        "magnitude": 1.1918652057647705,
        "relative": 0.009396243325148318,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.23.post_attention_layernorm.weight": {
        "magnitude": 0.0014023154508322477,
        "relative": 3.524475901772357e-05,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.23.self_attn.k_proj.bias": {
        "magnitude": 0.002207361627370119,
        "relative": 0.00011310496924926605,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.23.self_attn.k_proj.weight": {
        "magnitude": 0.17689870297908783,
        "relative": 0.008121478844458655,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.23.self_attn.o_proj.weight": {
        "magnitude": 0.4828506410121918,
        "relative": 0.010207862972575947,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.23.self_attn.q_proj.bias": {
        "magnitude": 0.005685432814061642,
        "relative": 8.733362448682985e-05,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.23.self_attn.q_proj.weight": {
        "magnitude": 0.5095850229263306,
        "relative": 0.009241033505968324,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.23.self_attn.v_proj.bias": {
        "magnitude": 0.0014625393087044358,
        "relative": 0.0014479020106370004,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.23.self_attn.v_proj.weight": {
        "magnitude": 0.17285527288913727,
        "relative": 0.010192741208528845,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.24.input_layernorm.weight": {
        "magnitude": 0.0019067992689087987,
        "relative": 4.112471878557467e-05,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "layernorm"
      },
      "model.layers.24.mlp.down_proj.weight": {
        "magnitude": 1.1850589513778687,
        "relative": 0.009800106782452864,
        "shape": [
          2048,
          11008
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.24.mlp.gate_proj.weight": {
        "magnitude": 1.2299854755401611,
        "relative": 0.010089672161889166,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.24.mlp.up_proj.weight": {
        "magnitude": 1.1929289102554321,
        "relative": 0.00951409750227066,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.24.post_attention_layernorm.weight": {
        "magnitude": 0.0013830858515575528,
        "relative": 3.539792361605639e-05,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.24.self_attn.k_proj.bias": {
        "magnitude": 0.0020154747180640697,
        "relative": 0.00014643962286410662,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.24.self_attn.k_proj.weight": {
        "magnitude": 0.17722582817077637,
        "relative": 0.008977524150041731,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.24.self_attn.o_proj.weight": {
        "magnitude": 0.5060631036758423,
        "relative": 0.010051438347675822,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.24.self_attn.q_proj.bias": {
        "magnitude": 0.005023538134992123,
        "relative": 6.833289559349723e-05,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.24.self_attn.q_proj.weight": {
        "magnitude": 0.5238742232322693,
        "relative": 0.009566464005508315,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.24.self_attn.v_proj.bias": {
        "magnitude": 0.001345082768239081,
        "relative": 0.0018985296699719962,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.24.self_attn.v_proj.weight": {
        "magnitude": 0.16864874958992004,
        "relative": 0.009017474807433618,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.25.input_layernorm.weight": {
        "magnitude": 0.0010225895093753934,
        "relative": 1.87468025861281e-05,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "layernorm"
      },
      "model.layers.25.mlp.down_proj.weight": {
        "magnitude": 1.1808288097381592,
        "relative": 0.009617698598962708,
        "shape": [
          2048,
          11008
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.25.mlp.gate_proj.weight": {
        "magnitude": 1.2393838167190552,
        "relative": 0.010064174372184232,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.25.mlp.up_proj.weight": {
        "magnitude": 1.1966708898544312,
        "relative": 0.009397279400815193,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.25.post_attention_layernorm.weight": {
        "magnitude": 5.269050598144531e-05,
        "relative": 1.358443232065075e-06,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.25.self_attn.k_proj.bias": {
        "magnitude": 0.0018565626814961433,
        "relative": 0.00012563489158017615,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.25.self_attn.k_proj.weight": {
        "magnitude": 0.17717814445495605,
        "relative": 0.011421472155585588,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.25.self_attn.o_proj.weight": {
        "magnitude": 0.48927491903305054,
        "relative": 0.009402848390334692,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.25.self_attn.q_proj.bias": {
        "magnitude": 0.006761608179658651,
        "relative": 0.00011403737823111448,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.25.self_attn.q_proj.weight": {
        "magnitude": 0.5261643528938293,
        "relative": 0.010045136065417256,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.25.self_attn.v_proj.bias": {
        "magnitude": 0.0012899445137009025,
        "relative": 0.00143129417681989,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.25.self_attn.v_proj.weight": {
        "magnitude": 0.16579578816890717,
        "relative": 0.00883225525219159,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.26.input_layernorm.weight": {
        "magnitude": 0.001953125,
        "relative": 4.094543979302272e-05,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "layernorm"
      },
      "model.layers.26.mlp.down_proj.weight": {
        "magnitude": 1.1664106845855713,
        "relative": 0.00933090788854002,
        "shape": [
          2048,
          11008
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.26.mlp.gate_proj.weight": {
        "magnitude": 1.2319391965866089,
        "relative": 0.010064962969442766,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.26.mlp.up_proj.weight": {
        "magnitude": 1.1857644319534302,
        "relative": 0.009109281215100953,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.26.post_attention_layernorm.weight": {
        "magnitude": 0.00022459030151367188,
        "relative": 5.752097352444778e-06,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.26.self_attn.k_proj.bias": {
        "magnitude": 0.0018779006786644459,
        "relative": 0.00014055089698679815,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.26.self_attn.k_proj.weight": {
        "magnitude": 0.18016742169857025,
        "relative": 0.009536426159585222,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.26.self_attn.o_proj.weight": {
        "magnitude": 0.5158326029777527,
        "relative": 0.009756980485048078,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.26.self_attn.q_proj.bias": {
        "magnitude": 0.0048180874437093735,
        "relative": 6.983395621953882e-05,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.26.self_attn.q_proj.weight": {
        "magnitude": 0.5308736562728882,
        "relative": 0.009863726449034846,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.26.self_attn.v_proj.bias": {
        "magnitude": 0.0013663547579199076,
        "relative": 0.001782314249176085,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.26.self_attn.v_proj.weight": {
        "magnitude": 0.17522203922271729,
        "relative": 0.008374519506424354,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.27.input_layernorm.weight": {
        "magnitude": 0.0010082157095894217,
        "relative": 1.9790283980897105e-05,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "layernorm"
      },
      "model.layers.27.mlp.down_proj.weight": {
        "magnitude": 1.1456011533737183,
        "relative": 0.00915923005658143,
        "shape": [
          2048,
          11008
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.27.mlp.gate_proj.weight": {
        "magnitude": 1.214921236038208,
        "relative": 0.009728266881604183,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.27.mlp.up_proj.weight": {
        "magnitude": 1.1675766706466675,
        "relative": 0.008979006669115512,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.27.post_attention_layernorm.weight": {
        "magnitude": 0.0010034629376605153,
        "relative": 2.4751439395347636e-05,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.27.self_attn.k_proj.bias": {
        "magnitude": 0.0027455645613372326,
        "relative": 3.992094477877582e-05,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.27.self_attn.k_proj.weight": {
        "magnitude": 0.17540964484214783,
        "relative": 0.010898683357352246,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.27.self_attn.o_proj.weight": {
        "magnitude": 0.48483002185821533,
        "relative": 0.009527992363605806,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.27.self_attn.q_proj.bias": {
        "magnitude": 0.0045689549297094345,
        "relative": 2.0225231303642655e-05,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.27.self_attn.q_proj.weight": {
        "magnitude": 0.513719379901886,
        "relative": 0.010041427279880319,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.27.self_attn.v_proj.bias": {
        "magnitude": 0.001591709558852017,
        "relative": 0.0008743467208769631,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.27.self_attn.v_proj.weight": {
        "magnitude": 0.1688905954360962,
        "relative": 0.008468492612830735,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.28.input_layernorm.weight": {
        "magnitude": 0.0010144512634724379,
        "relative": 2.2376283983317572e-05,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "layernorm"
      },
      "model.layers.28.mlp.down_proj.weight": {
        "magnitude": 1.1526515483856201,
        "relative": 0.00917006787752686,
        "shape": [
          2048,
          11008
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.28.mlp.gate_proj.weight": {
        "magnitude": 1.2049113512039185,
        "relative": 0.009518438476788665,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.28.mlp.up_proj.weight": {
        "magnitude": 1.160204529762268,
        "relative": 0.008903752795382967,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.28.post_attention_layernorm.weight": {
        "magnitude": 0.00020179152488708496,
        "relative": 4.8087101756034885e-06,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.28.self_attn.k_proj.bias": {
        "magnitude": 0.002363684354349971,
        "relative": 0.0001782910238445271,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.28.self_attn.k_proj.weight": {
        "magnitude": 0.17554165422916412,
        "relative": 0.010205806682083377,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.28.self_attn.o_proj.weight": {
        "magnitude": 0.5145972371101379,
        "relative": 0.009691744536686166,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.28.self_attn.q_proj.bias": {
        "magnitude": 0.004964016377925873,
        "relative": 7.946480926343568e-05,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.28.self_attn.q_proj.weight": {
        "magnitude": 0.5161981582641602,
        "relative": 0.009743703861770788,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.28.self_attn.v_proj.bias": {
        "magnitude": 0.0012125023640692234,
        "relative": 0.0010266856056201762,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.28.self_attn.v_proj.weight": {
        "magnitude": 0.16420204937458038,
        "relative": 0.007507381576479953,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.29.input_layernorm.weight": {
        "magnitude": 0.00034854415571317077,
        "relative": 7.331528748902143e-06,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "layernorm"
      },
      "model.layers.29.mlp.down_proj.weight": {
        "magnitude": 1.1309398412704468,
        "relative": 0.00894954837732269,
        "shape": [
          2048,
          11008
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.29.mlp.gate_proj.weight": {
        "magnitude": 1.1909691095352173,
        "relative": 0.009414662006378702,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.29.mlp.up_proj.weight": {
        "magnitude": 1.1411560773849487,
        "relative": 0.008727344394717862,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.29.post_attention_layernorm.weight": {
        "magnitude": 0.0002886056900024414,
        "relative": 6.505959102348558e-06,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.29.self_attn.k_proj.bias": {
        "magnitude": 0.001943055889569223,
        "relative": 7.20483458643441e-05,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.29.self_attn.k_proj.weight": {
        "magnitude": 0.17795497179031372,
        "relative": 0.010459164178000144,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.29.self_attn.o_proj.weight": {
        "magnitude": 0.5155231356620789,
        "relative": 0.009743893710919672,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.29.self_attn.q_proj.bias": {
        "magnitude": 0.005280770361423492,
        "relative": 8.917731340584907e-05,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.29.self_attn.q_proj.weight": {
        "magnitude": 0.5221458077430725,
        "relative": 0.010067692740532059,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.29.self_attn.v_proj.bias": {
        "magnitude": 0.0013989964500069618,
        "relative": 0.0009477152437612554,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.29.self_attn.v_proj.weight": {
        "magnitude": 0.16439734399318695,
        "relative": 0.0075441535656967955,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.3.input_layernorm.weight": {
        "magnitude": 0.000857583130709827,
        "relative": 2.776558079272929e-05,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "layernorm"
      },
      "model.layers.3.mlp.down_proj.weight": {
        "magnitude": 0.484470933675766,
        "relative": 0.005947183328084398,
        "shape": [
          2048,
          11008
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.3.mlp.gate_proj.weight": {
        "magnitude": 0.4804632365703583,
        "relative": 0.006107217077880436,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.3.mlp.up_proj.weight": {
        "magnitude": 0.4361305236816406,
        "relative": 0.005117722981744265,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.3.post_attention_layernorm.weight": {
        "magnitude": 9.41530815907754e-05,
        "relative": 1.5433624582583263e-06,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.3.self_attn.k_proj.bias": {
        "magnitude": 0.0020537672098726034,
        "relative": 0.00013932105749716917,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.3.self_attn.k_proj.weight": {
        "magnitude": 0.11411373317241669,
        "relative": 0.004344435292007054,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.3.self_attn.o_proj.weight": {
        "magnitude": 0.34413138031959534,
        "relative": 0.007588092964391946,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.3.self_attn.q_proj.bias": {
        "magnitude": 0.004565845243632793,
        "relative": 7.399624434942203e-05,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.3.self_attn.q_proj.weight": {
        "magnitude": 0.3465837240219116,
        "relative": 0.0058919341183004,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.3.self_attn.v_proj.bias": {
        "magnitude": 0.0013859247555956244,
        "relative": 0.002295467943233504,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.3.self_attn.v_proj.weight": {
        "magnitude": 0.10851431638002396,
        "relative": 0.009104257239789306,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.30.input_layernorm.weight": {
        "magnitude": 0.000244140625,
        "relative": 5.533340058545736e-06,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "layernorm"
      },
      "model.layers.30.mlp.down_proj.weight": {
        "magnitude": 1.1027889251708984,
        "relative": 0.008674319588101005,
        "shape": [
          2048,
          11008
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.30.mlp.gate_proj.weight": {
        "magnitude": 1.1714338064193726,
        "relative": 0.009381805668531152,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.30.mlp.up_proj.weight": {
        "magnitude": 1.1217939853668213,
        "relative": 0.008482524899606448,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.30.post_attention_layernorm.weight": {
        "magnitude": 3.4332275390625e-05,
        "relative": 7.431576851011216e-07,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.30.self_attn.k_proj.bias": {
        "magnitude": 0.002021030755713582,
        "relative": 0.00019323393948384614,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.30.self_attn.k_proj.weight": {
        "magnitude": 0.17689920961856842,
        "relative": 0.011582424720531878,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.30.self_attn.o_proj.weight": {
        "magnitude": 0.4884941577911377,
        "relative": 0.008971226227696163,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.30.self_attn.q_proj.bias": {
        "magnitude": 0.005713979247957468,
        "relative": 8.898713459717932e-05,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.30.self_attn.q_proj.weight": {
        "magnitude": 0.5173230767250061,
        "relative": 0.009764091912918788,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.30.self_attn.v_proj.bias": {
        "magnitude": 0.0011538324179127812,
        "relative": 0.0007328200607489687,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.30.self_attn.v_proj.weight": {
        "magnitude": 0.15809476375579834,
        "relative": 0.007242414382742467,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.31.input_layernorm.weight": {
        "magnitude": 0.0001220703125,
        "relative": 2.8624893220199545e-06,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "layernorm"
      },
      "model.layers.31.mlp.down_proj.weight": {
        "magnitude": 1.0502749681472778,
        "relative": 0.00807561448172931,
        "shape": [
          2048,
          11008
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.31.mlp.gate_proj.weight": {
        "magnitude": 1.1248313188552856,
        "relative": 0.009112341808380123,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.31.mlp.up_proj.weight": {
        "magnitude": 1.0750133991241455,
        "relative": 0.007964951867421445,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.31.post_attention_layernorm.weight": {
        "magnitude": 6.400048732757568e-05,
        "relative": 1.2827241044921577e-06,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.31.self_attn.k_proj.bias": {
        "magnitude": 0.00175681640394032,
        "relative": 9.659115067814127e-05,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.31.self_attn.k_proj.weight": {
        "magnitude": 0.15325140953063965,
        "relative": 0.009757699049281272,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.31.self_attn.o_proj.weight": {
        "magnitude": 0.4182291626930237,
        "relative": 0.007694217399701756,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.31.self_attn.q_proj.bias": {
        "magnitude": 0.005341264419257641,
        "relative": 9.867441797616277e-05,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.31.self_attn.q_proj.weight": {
        "magnitude": 0.4814123511314392,
        "relative": 0.008937078494686092,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.31.self_attn.v_proj.bias": {
        "magnitude": 0.0011300097685307264,
        "relative": 0.00015326919050812407,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.31.self_attn.v_proj.weight": {
        "magnitude": 0.14647038280963898,
        "relative": 0.0071374742903301015,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.32.input_layernorm.weight": {
        "magnitude": 3.0517578125e-05,
        "relative": 6.27276917068474e-07,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "layernorm"
      },
      "model.layers.32.mlp.down_proj.weight": {
        "magnitude": 1.0397309064865112,
        "relative": 0.007904847897056111,
        "shape": [
          2048,
          11008
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.32.mlp.gate_proj.weight": {
        "magnitude": 1.098858118057251,
        "relative": 0.00885536250960629,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.32.mlp.up_proj.weight": {
        "magnitude": 1.059145450592041,
        "relative": 0.007857553272278966,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.32.post_attention_layernorm.weight": {
        "magnitude": 0.00023731299734208733,
        "relative": 4.49822377167081e-06,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.32.self_attn.k_proj.bias": {
        "magnitude": 0.002175381174311042,
        "relative": 0.00012432877029797302,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.32.self_attn.k_proj.weight": {
        "magnitude": 0.15737684071063995,
        "relative": 0.011065690358428366,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.32.self_attn.o_proj.weight": {
        "magnitude": 0.4324755370616913,
        "relative": 0.0070858767168712825,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.32.self_attn.q_proj.bias": {
        "magnitude": 0.006166837643831968,
        "relative": 0.00012042035006153334,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.32.self_attn.q_proj.weight": {
        "magnitude": 0.4897116720676422,
        "relative": 0.00917688042899294,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.32.self_attn.v_proj.bias": {
        "magnitude": 0.0010461611673235893,
        "relative": 0.0001392988768169631,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.32.self_attn.v_proj.weight": {
        "magnitude": 0.13919122517108917,
        "relative": 0.006242421251056786,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.33.input_layernorm.weight": {
        "magnitude": 0.00013828277587890625,
        "relative": 2.5138263914330847e-06,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "layernorm"
      },
      "model.layers.33.mlp.down_proj.weight": {
        "magnitude": 1.0142145156860352,
        "relative": 0.007638695912325414,
        "shape": [
          2048,
          11008
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.33.mlp.gate_proj.weight": {
        "magnitude": 1.0697071552276611,
        "relative": 0.00872142726978198,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.33.mlp.up_proj.weight": {
        "magnitude": 1.0338780879974365,
        "relative": 0.0075855908273319795,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.33.post_attention_layernorm.weight": {
        "magnitude": 0.00020983489230275154,
        "relative": 3.952428977544812e-06,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.33.self_attn.k_proj.bias": {
        "magnitude": 0.0019687532912939787,
        "relative": 0.00014534963189564935,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.33.self_attn.k_proj.weight": {
        "magnitude": 0.16318298876285553,
        "relative": 0.012364730307840828,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.33.self_attn.o_proj.weight": {
        "magnitude": 0.4400891363620758,
        "relative": 0.0066831878931621486,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.33.self_attn.q_proj.bias": {
        "magnitude": 0.006310540717095137,
        "relative": 0.00011953942681678158,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.33.self_attn.q_proj.weight": {
        "magnitude": 0.49149689078330994,
        "relative": 0.010144598216996863,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.33.self_attn.v_proj.bias": {
        "magnitude": 0.0009759662789292634,
        "relative": 0.0001628324595398333,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.33.self_attn.v_proj.weight": {
        "magnitude": 0.12264233082532883,
        "relative": 0.0034835326349798488,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.34.input_layernorm.weight": {
        "magnitude": 0.0,
        "relative": 0.0,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "layernorm"
      },
      "model.layers.34.mlp.down_proj.weight": {
        "magnitude": 0.9544066190719604,
        "relative": 0.007361708680089231,
        "shape": [
          2048,
          11008
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.34.mlp.gate_proj.weight": {
        "magnitude": 1.0350269079208374,
        "relative": 0.00842174153953759,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.34.mlp.up_proj.weight": {
        "magnitude": 0.9996655583381653,
        "relative": 0.007439695156888105,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.34.post_attention_layernorm.weight": {
        "magnitude": 0.000376903306460008,
        "relative": 6.190886568878005e-06,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.34.self_attn.k_proj.bias": {
        "magnitude": 0.001467701978981495,
        "relative": 7.862433158912307e-05,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.34.self_attn.k_proj.weight": {
        "magnitude": 0.15949112176895142,
        "relative": 0.011186148052036016,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.34.self_attn.o_proj.weight": {
        "magnitude": 0.3918239176273346,
        "relative": 0.00705709162705651,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.34.self_attn.q_proj.bias": {
        "magnitude": 0.00493597099557519,
        "relative": 8.69654318907654e-05,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.34.self_attn.q_proj.weight": {
        "magnitude": 0.47860920429229736,
        "relative": 0.010305905895756208,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.34.self_attn.v_proj.bias": {
        "magnitude": 0.001169078634120524,
        "relative": 0.00012150465916324153,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.34.self_attn.v_proj.weight": {
        "magnitude": 0.1381809115409851,
        "relative": 0.005162889056443299,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.35.input_layernorm.weight": {
        "magnitude": 0.0006905339541845024,
        "relative": 1.1369781936171144e-05,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "layernorm"
      },
      "model.layers.35.mlp.down_proj.weight": {
        "magnitude": 0.8939603567123413,
        "relative": 0.007652881676731175,
        "shape": [
          2048,
          11008
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.35.mlp.gate_proj.weight": {
        "magnitude": 1.001888632774353,
        "relative": 0.007988515943387512,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.35.mlp.up_proj.weight": {
        "magnitude": 0.9573116898536682,
        "relative": 0.007387803391907582,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.35.post_attention_layernorm.weight": {
        "magnitude": 0.000213623046875,
        "relative": 2.7983548468187696e-06,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.35.self_attn.k_proj.bias": {
        "magnitude": 0.0014248801162466407,
        "relative": 5.866410053704861e-05,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.35.self_attn.k_proj.weight": {
        "magnitude": 0.15855802595615387,
        "relative": 0.011211868015090694,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.35.self_attn.o_proj.weight": {
        "magnitude": 0.37808850407600403,
        "relative": 0.006750211539612586,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.35.self_attn.q_proj.bias": {
        "magnitude": 0.004487580619752407,
        "relative": 9.132321586079536e-05,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.35.self_attn.q_proj.weight": {
        "magnitude": 0.462459921836853,
        "relative": 0.01002723397352333,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.35.self_attn.v_proj.bias": {
        "magnitude": 0.001081760972738266,
        "relative": 8.163214937224574e-05,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.35.self_attn.v_proj.weight": {
        "magnitude": 0.14553798735141754,
        "relative": 0.005740944266754172,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.4.input_layernorm.weight": {
        "magnitude": 0.00039147151983343065,
        "relative": 1.7076652065105633e-05,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "layernorm"
      },
      "model.layers.4.mlp.down_proj.weight": {
        "magnitude": 0.5522491931915283,
        "relative": 0.006599107236654891,
        "shape": [
          2048,
          11008
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.4.mlp.gate_proj.weight": {
        "magnitude": 0.5504616498947144,
        "relative": 0.0061891946318951155,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.4.mlp.up_proj.weight": {
        "magnitude": 0.516025722026825,
        "relative": 0.0060727184748559,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.4.post_attention_layernorm.weight": {
        "magnitude": 0.000160607261932455,
        "relative": 3.21001065145634e-06,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.4.self_attn.k_proj.bias": {
        "magnitude": 0.002376456279307604,
        "relative": 0.0002272736408881514,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.4.self_attn.k_proj.weight": {
        "magnitude": 0.09327588975429535,
        "relative": 0.003729135912129463,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.4.self_attn.o_proj.weight": {
        "magnitude": 0.34367483854293823,
        "relative": 0.0071773331160180105,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.4.self_attn.q_proj.bias": {
        "magnitude": 0.0035084544215351343,
        "relative": 5.3821384808730655e-05,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.4.self_attn.q_proj.weight": {
        "magnitude": 0.32676640152931213,
        "relative": 0.005517217603473737,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.4.self_attn.v_proj.bias": {
        "magnitude": 0.0013404909987002611,
        "relative": 0.0021179478480716878,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.4.self_attn.v_proj.weight": {
        "magnitude": 0.0982486754655838,
        "relative": 0.006228611876140237,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.5.input_layernorm.weight": {
        "magnitude": 0.0004518860368989408,
        "relative": 1.6247689234235615e-05,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "layernorm"
      },
      "model.layers.5.mlp.down_proj.weight": {
        "magnitude": 0.7221346497535706,
        "relative": 0.006622835155950861,
        "shape": [
          2048,
          11008
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.5.mlp.gate_proj.weight": {
        "magnitude": 0.7269935011863708,
        "relative": 0.006361413174626524,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.5.mlp.up_proj.weight": {
        "magnitude": 0.6805437207221985,
        "relative": 0.006157620927355312,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.5.post_attention_layernorm.weight": {
        "magnitude": 0.00022917226306162775,
        "relative": 4.8543240375531355e-06,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.5.self_attn.k_proj.bias": {
        "magnitude": 0.0021664928644895554,
        "relative": 0.0003004509833224169,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.5.self_attn.k_proj.weight": {
        "magnitude": 0.10653434693813324,
        "relative": 0.0044322603160855106,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.5.self_attn.o_proj.weight": {
        "magnitude": 0.3634319007396698,
        "relative": 0.007930393772945481,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.5.self_attn.q_proj.bias": {
        "magnitude": 0.004131990950554609,
        "relative": 5.5072529095610366e-05,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.5.self_attn.q_proj.weight": {
        "magnitude": 0.33511197566986084,
        "relative": 0.005773863022922842,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.5.self_attn.v_proj.bias": {
        "magnitude": 0.0011991072678938508,
        "relative": 0.0010764175147124017,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.5.self_attn.v_proj.weight": {
        "magnitude": 0.11463218182325363,
        "relative": 0.006640464095467428,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.6.input_layernorm.weight": {
        "magnitude": 0.00025153893511742353,
        "relative": 9.908317594780343e-06,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "layernorm"
      },
      "model.layers.6.mlp.down_proj.weight": {
        "magnitude": 0.7981162071228027,
        "relative": 0.0066623563729829895,
        "shape": [
          2048,
          11008
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.6.mlp.gate_proj.weight": {
        "magnitude": 0.8253841400146484,
        "relative": 0.006581293873221532,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.6.mlp.up_proj.weight": {
        "magnitude": 0.7675018906593323,
        "relative": 0.006347184611657903,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.6.post_attention_layernorm.weight": {
        "magnitude": 0.0001442139910068363,
        "relative": 2.8921877039985883e-06,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.6.self_attn.k_proj.bias": {
        "magnitude": 0.0022810741793364286,
        "relative": 0.00026159707538222826,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.6.self_attn.k_proj.weight": {
        "magnitude": 0.11152797937393188,
        "relative": 0.004736636364423079,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.6.self_attn.o_proj.weight": {
        "magnitude": 0.3508966267108917,
        "relative": 0.006921480035620845,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.6.self_attn.q_proj.bias": {
        "magnitude": 0.002657344564795494,
        "relative": 4.15590250397975e-05,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.6.self_attn.q_proj.weight": {
        "magnitude": 0.35080233216285706,
        "relative": 0.005996625362928289,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.6.self_attn.v_proj.bias": {
        "magnitude": 0.001347461948171258,
        "relative": 0.0030266083874970384,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.6.self_attn.v_proj.weight": {
        "magnitude": 0.11013395339250565,
        "relative": 0.006795627512674454,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.7.input_layernorm.weight": {
        "magnitude": 0.00020665477495640516,
        "relative": 6.7042648509944765e-06,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "layernorm"
      },
      "model.layers.7.mlp.down_proj.weight": {
        "magnitude": 0.812334418296814,
        "relative": 0.007050121904895546,
        "shape": [
          2048,
          11008
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.7.mlp.gate_proj.weight": {
        "magnitude": 0.8439520597457886,
        "relative": 0.006763129979707607,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.7.mlp.up_proj.weight": {
        "magnitude": 0.7836385369300842,
        "relative": 0.0067513404210309295,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.7.post_attention_layernorm.weight": {
        "magnitude": 0.0002891373005695641,
        "relative": 5.494547216270963e-06,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.7.self_attn.k_proj.bias": {
        "magnitude": 0.002742849523201585,
        "relative": 0.0003256829033178339,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.7.self_attn.k_proj.weight": {
        "magnitude": 0.1307816207408905,
        "relative": 0.006672167283636117,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.7.self_attn.o_proj.weight": {
        "magnitude": 0.3812883496284485,
        "relative": 0.007604822861435875,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.7.self_attn.q_proj.bias": {
        "magnitude": 0.0035608115140348673,
        "relative": 6.319707873411916e-05,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.7.self_attn.q_proj.weight": {
        "magnitude": 0.37986764311790466,
        "relative": 0.006899228636171784,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.7.self_attn.v_proj.bias": {
        "magnitude": 0.001318378490395844,
        "relative": 0.002981532596818046,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.7.self_attn.v_proj.weight": {
        "magnitude": 0.12993144989013672,
        "relative": 0.007262439394766393,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.8.input_layernorm.weight": {
        "magnitude": 0.000336102006258443,
        "relative": 1.0698057275823273e-05,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "layernorm"
      },
      "model.layers.8.mlp.down_proj.weight": {
        "magnitude": 0.8931675553321838,
        "relative": 0.007477680053591478,
        "shape": [
          2048,
          11008
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.8.mlp.gate_proj.weight": {
        "magnitude": 0.934978187084198,
        "relative": 0.0065994640774781,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.8.mlp.up_proj.weight": {
        "magnitude": 0.8661984205245972,
        "relative": 0.007178212717409768,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.8.post_attention_layernorm.weight": {
        "magnitude": 6.806663441238925e-05,
        "relative": 1.3124910809177807e-06,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.8.self_attn.k_proj.bias": {
        "magnitude": 0.002435846719890833,
        "relative": 0.0003044367452047557,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.8.self_attn.k_proj.weight": {
        "magnitude": 0.13453905284404755,
        "relative": 0.006181674711611827,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.8.self_attn.o_proj.weight": {
        "magnitude": 0.37888434529304504,
        "relative": 0.00741011414263898,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.8.self_attn.q_proj.bias": {
        "magnitude": 0.003108880715444684,
        "relative": 5.257651226611137e-05,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.8.self_attn.q_proj.weight": {
        "magnitude": 0.39195719361305237,
        "relative": 0.0070085113328397855,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.8.self_attn.v_proj.bias": {
        "magnitude": 0.0013772048987448215,
        "relative": 0.0030061516753693298,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.8.self_attn.v_proj.weight": {
        "magnitude": 0.1294490545988083,
        "relative": 0.007182828533393047,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.9.input_layernorm.weight": {
        "magnitude": 0.00012862941366620362,
        "relative": 4.672392338486038e-06,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "layernorm"
      },
      "model.layers.9.mlp.down_proj.weight": {
        "magnitude": 0.8925129175186157,
        "relative": 0.007665841878949474,
        "shape": [
          2048,
          11008
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.9.mlp.gate_proj.weight": {
        "magnitude": 0.9531436562538147,
        "relative": 0.0066473544125914144,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.9.mlp.up_proj.weight": {
        "magnitude": 0.8739418983459473,
        "relative": 0.00737087238259033,
        "shape": [
          11008,
          2048
        ],
        "param_count": 22544384,
        "layer_type": "mlp"
      },
      "model.layers.9.post_attention_layernorm.weight": {
        "magnitude": 0.0002429446903988719,
        "relative": 4.942015451300284e-06,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.9.self_attn.k_proj.bias": {
        "magnitude": 0.002646949142217636,
        "relative": 0.00029970928222599686,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.9.self_attn.k_proj.weight": {
        "magnitude": 0.14078041911125183,
        "relative": 0.005875799517067672,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.layers.9.self_attn.o_proj.weight": {
        "magnitude": 0.41558289527893066,
        "relative": 0.008092876948507813,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.9.self_attn.q_proj.bias": {
        "magnitude": 0.0036878495011478662,
        "relative": 5.5214426086344156e-05,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "attention"
      },
      "model.layers.9.self_attn.q_proj.weight": {
        "magnitude": 0.41921350359916687,
        "relative": 0.00725551283608862,
        "shape": [
          2048,
          2048
        ],
        "param_count": 4194304,
        "layer_type": "attention"
      },
      "model.layers.9.self_attn.v_proj.bias": {
        "magnitude": 0.0012433251831680536,
        "relative": 0.0018810862605187705,
        "shape": [
          256
        ],
        "param_count": 256,
        "layer_type": "attention"
      },
      "model.layers.9.self_attn.v_proj.weight": {
        "magnitude": 0.13650156557559967,
        "relative": 0.00730001923529231,
        "shape": [
          256,
          2048
        ],
        "param_count": 524288,
        "layer_type": "attention"
      },
      "model.norm.weight": {
        "magnitude": 0.0,
        "relative": 0.0,
        "shape": [
          2048
        ],
        "param_count": 2048,
        "layer_type": "other"
      }
    }
  },
  "top_changes": {
    "cpt": {
      "top_10": [
        [
          "lm_head.weight",
          28.570758819580078
        ],
        [
          "model.embed_tokens.weight",
          28.570758819580078
        ],
        [
          "model.layers.22.mlp.gate_proj.weight",
          10.912964820861816
        ],
        [
          "model.layers.21.mlp.gate_proj.weight",
          10.824860572814941
        ],
        [
          "model.layers.23.mlp.gate_proj.weight",
          10.514274597167969
        ],
        [
          "model.layers.24.mlp.gate_proj.weight",
          10.470571517944336
        ],
        [
          "model.layers.20.mlp.gate_proj.weight",
          10.41576862335205
        ],
        [
          "model.layers.26.mlp.gate_proj.weight",
          10.344715118408203
        ],
        [
          "model.layers.25.mlp.gate_proj.weight",
          10.323164939880371
        ],
        [
          "model.layers.18.mlp.gate_proj.weight",
          9.979236602783203
        ]
      ],
      "bottom_10": [
        [
          "model.layers.31.self_attn.v_proj.bias",
          0.003029789077118039
        ],
        [
          "model.layers.2.post_attention_layernorm.weight",
          0.0025318043772131205
        ],
        [
          "model.layers.33.self_attn.v_proj.bias",
          0.0023081155959516764
        ],
        [
          "model.layers.34.self_attn.v_proj.bias",
          0.0021383718121796846
        ],
        [
          "model.layers.3.post_attention_layernorm.weight",
          0.0019772194791585207
        ],
        [
          "model.layers.33.post_attention_layernorm.weight",
          0.0019533676095306873
        ],
        [
          "model.layers.35.post_attention_layernorm.weight",
          0.0016806847415864468
        ],
        [
          "model.layers.35.self_attn.v_proj.bias",
          0.001465479377657175
        ],
        [
          "model.norm.weight",
          0.0008802615338936448
        ],
        [
          "model.layers.34.post_attention_layernorm.weight",
          0.00017674425907898694
        ]
      ]
    },
    "sft": {
      "top_10": [
        [
          "lm_head.weight",
          30.050071716308594
        ],
        [
          "model.embed_tokens.weight",
          30.050071716308594
        ],
        [
          "model.layers.22.mlp.gate_proj.weight",
          11.006889343261719
        ],
        [
          "model.layers.21.mlp.gate_proj.weight",
          10.914824485778809
        ],
        [
          "model.layers.23.mlp.gate_proj.weight",
          10.61266040802002
        ],
        [
          "model.layers.24.mlp.gate_proj.weight",
          10.57171630859375
        ],
        [
          "model.layers.20.mlp.gate_proj.weight",
          10.507711410522461
        ],
        [
          "model.layers.26.mlp.gate_proj.weight",
          10.452475547790527
        ],
        [
          "model.layers.25.mlp.gate_proj.weight",
          10.428874015808105
        ],
        [
          "model.layers.18.mlp.gate_proj.weight",
          10.070116996765137
        ]
      ],
      "bottom_10": [
        [
          "model.layers.31.self_attn.v_proj.bias",
          0.003072296967729926
        ],
        [
          "model.layers.2.post_attention_layernorm.weight",
          0.0025244499556720257
        ],
        [
          "model.layers.33.self_attn.v_proj.bias",
          0.0024930264335125685
        ],
        [
          "model.layers.34.self_attn.v_proj.bias",
          0.0023262680042535067
        ],
        [
          "model.layers.3.post_attention_layernorm.weight",
          0.001975304214283824
        ],
        [
          "model.layers.33.post_attention_layernorm.weight",
          0.001961431233212352
        ],
        [
          "model.layers.35.self_attn.v_proj.bias",
          0.0016065990785136819
        ],
        [
          "model.layers.35.post_attention_layernorm.weight",
          0.0015869140625
        ],
        [
          "model.norm.weight",
          0.0008802615338936448
        ],
        [
          "model.layers.34.post_attention_layernorm.weight",
          0.00036162094329483807
        ]
      ]
    },
    "cpt_to_sft": {
      "top_10": [
        [
          "lm_head.weight",
          4.074172019958496
        ],
        [
          "model.embed_tokens.weight",
          4.074172019958496
        ],
        [
          "model.layers.25.mlp.gate_proj.weight",
          1.2393838167190552
        ],
        [
          "model.layers.26.mlp.gate_proj.weight",
          1.2319391965866089
        ],
        [
          "model.layers.24.mlp.gate_proj.weight",
          1.2299854755401611
        ],
        [
          "model.layers.22.mlp.gate_proj.weight",
          1.2281211614608765
        ],
        [
          "model.layers.23.mlp.gate_proj.weight",
          1.226951241493225
        ],
        [
          "model.layers.21.mlp.gate_proj.weight",
          1.219687819480896
        ],
        [
          "model.layers.27.mlp.gate_proj.weight",
          1.214921236038208
        ],
        [
          "model.layers.20.mlp.gate_proj.weight",
          1.2074999809265137
        ]
      ],
      "bottom_10": [
        [
          "model.layers.31.post_attention_layernorm.weight",
          6.400048732757568e-05
        ],
        [
          "model.layers.25.post_attention_layernorm.weight",
          5.269050598144531e-05
        ],
        [
          "model.layers.10.post_attention_layernorm.weight",
          4.9736074288375676e-05
        ],
        [
          "model.layers.30.post_attention_layernorm.weight",
          3.4332275390625e-05
        ],
        [
          "model.layers.32.input_layernorm.weight",
          3.0517578125e-05
        ],
        [
          "model.layers.16.post_attention_layernorm.weight",
          8.821487426757812e-06
        ],
        [
          "model.layers.12.post_attention_layernorm.weight",
          1.1920928955078125e-06
        ],
        [
          "model.layers.0.input_layernorm.weight",
          0.0
        ],
        [
          "model.layers.34.input_layernorm.weight",
          0.0
        ],
        [
          "model.norm.weight",
          0.0
        ]
      ]
    }
  }
}